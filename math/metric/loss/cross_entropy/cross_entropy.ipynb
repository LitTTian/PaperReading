{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cbb53c",
   "metadata": {},
   "source": [
    "<!-- #### Example: Why Cross-Entropy is optimal -->\n",
    "#### 示例：为什么交叉熵是最优的\n",
    "<!-- - for bianry classification, $t_i$ is either 0 or 1.\n",
    "- interpret the output $f(x_i)$ as the prob of the true value being 1 - 把模型的输出$f(x_i)$解释为真值为1的概率 -->\n",
    "- 对于一个二分类问题，$t_i$只能是0或1。我们将模型的输出$f(x_i)$解释为真值为1的概率。\n",
    "\n",
    "> $$\\begin{align*}\n",
    "> P(1|f(x_i)) &= f(x_i) \\\\\n",
    "> P(0|f(x_i)) &= 1 - f(x_i) \\\\\n",
    "> P(t_i|f(x_i)) &= f(x_i)^{t_i} (1 - f(x_i))^{(1-t_i)} \\qquad \\text{Since } t_i \\in \\{0, 1\\} \\\\\n",
    "> \\log P(\\{t_i\\}|f) &= \\sum_{i} t_i \\log f(x_i) + (1-t_i) \\log(1 - f(x_i)) \\\\\n",
    "> f_{\\text{ML}} &= \\argmax_{f\\in H} \\sum_{i} t_i \\log f(x_i) + (1-t_i) \\log(1 - f(x_i)) \\\\\n",
    "> \\text{可以} &\\text{ 被推广到多分类问题： } \\\\\n",
    "> f_{\\text{ML}} &= \\argmax_{f\\in H} \\sum_{i} \\sum_{j} t_{ij} \\log f_j(x_i) \\\\\n",
    "> \\end{align*}$$\n",
    "\n",
    "<!-- - **Key**: Minimizing Cross-Entropy is optimal for binary classification tasks. -->\n",
    "- **关键点**：最小化交叉熵对于二分类任务是最优的。\n",
    "\n",
    "<!-- - If we consider $p_i = \\left<t_i, 1-t_i\\right>$ and $q_i = \\left<f(x_i), 1-f(x_i)\\right>$, then the cross-entropy can be written as: -->\n",
    "- 如果我们考虑 $p_i = \\left<t_i, 1-t_i\\right>$ 和 $q_i = \\left<f(x_i), 1-f(x_i)\\right>$，那么交叉熵可以写成：\n",
    "$$\\begin{align*}\n",
    "\\log P(\\{t_i\\}|f) &= \\sum_{i} t_i \\log f(x_i) + (1-t_i) \\log(1 - f(x_i)) \\\\\n",
    "&= \\sum_{i}[(t_i(\\log f(x_i) - \\log(t_i)) + (1 - t_i) (\\log (1-f(x_i)) - \\log(1-t_i))  \\\\\n",
    "& \\qquad \\qquad \\qquad -(-t_i \\log(t_i) - (1-t_i) \\log(1-t_i) ))] \\\\\n",
    "&= \\sum_{i} D_{KL}(p_i||q_i) - H(p_i)\n",
    "\\end{align*}$$\n",
    "\n",
    "<!-- - since $H(p_i)$ is fixed, minimizing cross-entropy is equivalent to minimizing $\\sum_i D_{KL}(p_i||q_i)$\n",
    "- 对于分类问题，使用交叉熵损失函数的一个优势是：loss is unbounded, which can help the model to learn faster. -->\n",
    "- 由于 $H(p_i)$ 是固定的，最小化交叉熵等价于最小化 $\\sum_i D_{KL}(p_i||q_i)$。\n",
    "- 对于分类问题，使用交叉熵损失函数的一个优势是：损失函数是无界的，这可以帮助模型更快地学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb484caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 手动实现交叉熵 ===\n",
      "模型输出logits:\n",
      "[[2.  1.  0.1]\n",
      " [0.5 2.5 1. ]]\n",
      "真实标签（独热）:\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "Softmax概率分布:\n",
      "[[0.65900114 0.24243297 0.09856589]\n",
      " [0.09962365 0.73612472 0.16425163]]\n",
      "交叉熵损失值: 0.3617\n",
      "\n",
      "=== PyTorch API交叉熵 ===\n",
      "模型输出logits:\n",
      "tensor([[2.0000, 1.0000, 0.1000],\n",
      "        [0.5000, 2.5000, 1.0000]])\n",
      "真实标签（索引）: tensor([0, 1])\n",
      "Softmax概率分布:\n",
      "tensor([[0.6590, 0.2424, 0.0986],\n",
      "        [0.0996, 0.7361, 0.1643]])\n",
      "交叉熵损失值: 0.3617\n"
     ]
    }
   ],
   "source": [
    "# 一个代码案例\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ====================== 1. 手动实现交叉熵计算（基础版） ======================\n",
    "def soft_max(logits):\n",
    "    \"\"\"\n",
    "    手动实现Softmax函数\n",
    "    :param logits: 输入数组，shape=(batch_size, num_classes)\n",
    "    :return: Softmax概率分布，shape=(batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 减去最大值防止溢出\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return probs\n",
    "def cross_entropy_manual(logits, target):\n",
    "    \"\"\"\n",
    "    手动计算交叉熵（支持批量）\n",
    "    :param logits: 模型原始输出（未经过Softmax），shape=(batch_size, num_classes)\n",
    "    :param target: 真实标签（独热编码），shape=(batch_size, num_classes)\n",
    "    :return: 平均交叉熵损失\n",
    "    \"\"\"\n",
    "    # Step 1: Softmax转换为概率分布（防止数值溢出，减去每行最大值）\n",
    "    probs = soft_max(logits)\n",
    "    # Step 2: 计算交叉熵（逐样本）\n",
    "    # 加极小值防止log(0)\n",
    "    cross_entropy_per_sample = -np.sum(target * np.log(probs + 1e-10), axis=1)\n",
    "    # Step 3: 返回批量平均值\n",
    "    return np.mean(cross_entropy_per_sample)\n",
    "\n",
    "# ====================== 2. 固定参数设置（可自定义修改） ======================\n",
    "# 模型输出（logits）：假设是2个样本，3分类任务\n",
    "model_logits = np.array([\n",
    "    [2.0, 1.0, 0.1],  # 样本1的模型输出\n",
    "    [0.5, 2.5, 1.0]   # 样本2的模型输出\n",
    "])\n",
    "\n",
    "# 真实标签（两种形式）：\n",
    "# 形式1：独热编码（手动计算用）\n",
    "target_one_hot = np.array([\n",
    "    [1, 0, 0],  # 样本1真实类别：第0类\n",
    "    [0, 1, 0]   # 样本2真实类别：第1类\n",
    "])\n",
    "# 形式2：类别索引（PyTorch API用）\n",
    "target_index = torch.tensor([0, 1])\n",
    "\n",
    "# ====================== 3. 计算结果对比 ======================\n",
    "# 手动实现计算\n",
    "probs = soft_max(model_logits)\n",
    "ce_manual = cross_entropy_manual(model_logits, target_one_hot)\n",
    "print(\"=== 手动实现交叉熵 ===\")\n",
    "print(f\"模型输出logits:\\n{model_logits}\")\n",
    "print(f\"真实标签（独热）:\\n{target_one_hot}\")\n",
    "print(f\"Softmax概率分布:\\n{probs}\")\n",
    "print(f\"交叉熵损失值: {ce_manual:.4f}\\n\")\n",
    "\n",
    "# PyTorch API计算（推荐实际项目使用）\n",
    "# 注意：F.cross_entropy 内部自动做 Softmax + 交叉熵，输入直接用logits\n",
    "model_logits_torch = torch.tensor(model_logits, dtype=torch.float32)\n",
    "probs_torch = F.softmax(model_logits_torch, dim=1)\n",
    "ce_torch = F.cross_entropy(model_logits_torch, target_index)\n",
    "print(\"=== PyTorch API交叉熵 ===\")\n",
    "print(f\"模型输出logits:\\n{model_logits_torch}\")\n",
    "print(f\"真实标签（索引）: {target_index}\")\n",
    "print(f\"Softmax概率分布:\\n{probs_torch}\")\n",
    "print(f\"交叉熵损失值: {ce_torch.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
