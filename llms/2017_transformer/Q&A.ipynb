{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4a5323",
   "metadata": {},
   "source": [
    "### Masked Self-Attention中Mask是怎么实现的？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f4758",
   "metadata": {},
   "source": [
    "- 原始的注意力分数计算公式为：\n",
    "$$\\begin{align*}\n",
    "X &\\in \\mathbb{R}^{\\text{seq\\_len} \\times d_{model}} \\\\\n",
    "Q &= XW_Q, W_Q \\in \\mathbb{R}^{d_{model} \\times d_k} \\\\\n",
    "K &= XW_K, W_K \\in \\mathbb{R}^{d_{model} \\times d_k} \\\\\n",
    "V &= XW_V, W_V \\in \\mathbb{R}^{d_{model} \\times d_v} \\\\\n",
    "S &= \\frac{QK^T}{\\sqrt{d_k}}, S \\in \\mathbb{R}^{\\text{seq\\_len} \\times \\text{seq\\_len}} \\\\\n",
    "Attention(Q, K, V) &= softmax(S) V, Attention(Q, K, V) \\in \\mathbb{R}^{\\text{seq\\_len} \\times d_v}\n",
    "\\end{align*}$$\n",
    "- $d_v$一般等于$d_{model}$，保证输出Attension的维度和输入X的维度一致（便于后续的注意力或者FeedForward层）\n",
    "\n",
    "\n",
    "- 如果是Masked Self-Attention, 我们需要加上以一个mask掩码矩阵$M \\in \\mathbb{R}^{\\text{seq\\_len} \\times \\text{seq\\_len}}$，来屏蔽掉未来时刻的信息:\n",
    "$$\\begin{align*}\n",
    "S &= S + M \\\\\n",
    "M &= \\begin{bmatrix}\n",
    "0 & -inf & -inf & -inf \\\\\n",
    "0 & 0 & -inf & -inf \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "0 & 0 & 0 & -inf \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "- 然后再进行softmax计算，并乘以V:\n",
    "$$\\begin{align*}\n",
    "Attention(Q, K, V) = softmax(S) V\n",
    "\\end{align*}$$\n",
    "\n",
    "- 其中，每一行的计算都是可以并行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ea03e",
   "metadata": {},
   "source": [
    "## Cross-Attention中哪些信息来自别的块？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ba064",
   "metadata": {},
   "source": [
    "- 在传统的Encoder-Decoder架构中。\n",
    "  - Cross-Attention的Query来自Decoder的上一层(Masked Self-Attention)输出，\n",
    "  - 而Key和Value则来自Encoder的输出。(memory[batch_size, src_len, d_model]/encoder_output是整个句子通过Encoder层后的输出)\n",
    "    - key = memory * W_k\n",
    "    - value = memory * W_v\n",
    "    - W_K和W_V是cross atten层可训练的权重矩阵。\n",
    "  - 还有src_mask(源序列的mask), 用于处理源序列中的填充部分(padding)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f23d87",
   "metadata": {},
   "source": [
    "## 残差连接的作用?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932405b1",
   "metadata": {},
   "source": [
    "- 残差连接:\n",
    "  1. 缓解梯度消失：让梯度直接回传\n",
    "     - $y=F(x)$, 导数$d\\text{loss}/dx = d\\text{loss}/dy \\times dF(x)/dx$\n",
    "     - $y=x + F(x)$, 导数$d\\text{loss}/dx = d\\text{loss}/dy \\times (1 + dF(x)/dx)$\n",
    "     - 即使$dF(x)/dx$趋于0, 仍然保证$d\\text{loss}/dy$这部分\n",
    "     - 让100层网络的训练像10层网络一样稳定\n",
    "     - 底层参数也能获得有效的梯度更新\n",
    "  2. 保持信息流动：保留原始输入信息\n",
    "     - 每层输出 = 原始输入 + 学习到的变化, output = input + learned_change\n",
    "     - 恒等映射捷径：原始输入 x 直接传递到输出\n",
    "     - 选择性学习：网络只需要学习相对于输入的\"残差\" F(x)\n",
    "     - 信息无损传递：重要特征不会在深层中丢失\n",
    "  3. 训练稳定性：使深层网络更容易训练\n",
    "     - 深层网络容易出现的问题:\n",
    "       - Loss震荡剧烈\n",
    "       - 需要精心调参和学习率策略\n",
    "       - 对初始化敏感\n",
    "     - 训练过程更加稳定和平滑\n",
    "     - 对超参数的敏感性降低\n",
    "     - 可以使用更大的学习率加速收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb50a4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a24d7b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
