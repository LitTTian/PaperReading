{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94b4c13",
   "metadata": {},
   "source": [
    "### Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846aec40",
   "metadata": {},
   "source": [
    "```sh\n",
    "EncoderDecoder(\n",
    "  (encoder): Encoder(\n",
    "    (layers): ModuleList(\n",
    "      (0-1): 2 x EncoderLayer(\n",
    "        (self_attn): MultiHeadedAttention(\n",
    "          (linears): ModuleList(\n",
    "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
    "          )\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (feed_forward): PositionwiseFeedForward(\n",
    "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (sublayer): ModuleList(\n",
    "          (0-1): 2 x SublayerConnection(\n",
    "            (norm): LayerNorm()\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (norm): LayerNorm()\n",
    "  )\n",
    "  (decoder): Decoder(\n",
    "    (layers): ModuleList(\n",
    "      (0-1): 2 x DecoderLayer(\n",
    "        (self_attn): MultiHeadedAttention(\n",
    "          (linears): ModuleList(\n",
    "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
    "          )\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (src_attn): MultiHeadedAttention(\n",
    "          (linears): ModuleList(\n",
    "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
    "          )\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (feed_forward): PositionwiseFeedForward(\n",
    "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (sublayer): ModuleList(\n",
    "          (0-2): 3 x SublayerConnection(\n",
    "            (norm): LayerNorm()\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (norm): LayerNorm()\n",
    "  )\n",
    "  (src_embed): Sequential(\n",
    "    (0): Embeddings(\n",
    "      (lut): Embedding(11, 512)\n",
    "    )\n",
    "    (1): PositionalEncoding(\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "  )\n",
    "  (tgt_embed): Sequential(\n",
    "    (0): Embeddings(\n",
    "      (lut): Embedding(11, 512)\n",
    "    )\n",
    "    (1): PositionalEncoding(\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "  )\n",
    "  (generator): Generator(\n",
    "    (proj): Linear(in_features=512, out_features=11, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6a660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_model(\n",
    "#     src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, log_on = False\n",
    "# ):\n",
    "#     \"#SOL(辅助函数): 从超参数构建模型\"\n",
    "#     c = copy.deepcopy\n",
    "#     attn = MultiHeadedAttention(h, d_model)\n",
    "#     ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "#     position = PositionalEncoding(d_model, dropout)\n",
    "#     model = EncoderDecoder(\n",
    "#         Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), # HL: encoder\n",
    "#         Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), # HL: decoder\n",
    "#         nn.Sequential(Embeddings(d_model, src_vocab), c(position)), # HL: source embeddings\n",
    "#         nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), # HL: target embeddings\n",
    "#         Generator(d_model, tgt_vocab), # HL: generator\n",
    "#     )\n",
    "\n",
    "#     # See https://arxiv.org/pdf/1502.01852.pdf\n",
    "#     # NT: 初始化网络权重 (Glorot / fan_avg)\n",
    "#     for p in model.parameters():\n",
    "#         if p.dim() > 1:\n",
    "#             nn.init.xavier_uniform_(p) # [-limit, limit] (limit = \\sqrt(fan_in + fan_out))\n",
    "\n",
    "#     # ✅ 如果需要 logging，给每个子模块加 log wrapper\n",
    "#     if log_on:\n",
    "#         for name, module in model.named_modules():\n",
    "#             add_logging(module, enabled=True)\n",
    "#     return model\n",
    "\n",
    "from tools.make_model import make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfef2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[LOG] EncoderDecoder.forward 输入: [torch.Size([16, 128]), torch.Size([16, 128]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Sequential.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embeddings.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embedding.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embedding.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Sequential.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Encoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Encoder.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Sequential.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embeddings.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embedding.forward 输入: [torch.Size([16, 128])]\n",
      "[LOG] Embedding.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Sequential.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Decoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 8, 128, 128])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 8, 128, 128])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 2048])\n",
      "[LOG] Linear.forward 输入: [torch.Size([16, 128, 2048])]\n",
      "[LOG] Linear.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Dropout.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] Dropout.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] LayerNorm.forward 输入: [torch.Size([16, 128, 768])]\n",
      "[LOG] LayerNorm.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] Decoder.forward 输出: torch.Size([16, 128, 768])\n",
      "[LOG] EncoderDecoder.forward 输出: torch.Size([16, 128, 768])\n",
      "模型输出 shape: torch.Size([16, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 示例输入\n",
    "# sentence = \"I love machine learning\"\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Natural language processing is a fascinating field of study.\",\n",
    "    \"The weather is really nice outside this morning.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Natural language processing is a fascinating field of study.\",\n",
    "    \"The weather is really nice outside this morning.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Natural language processing is a fascinating field of study.\",\n",
    "    \"The weather is really nice outside this morning.\",\n",
    "    \"This is the sixteenth and final sentence in our batch.\",\n",
    "    \"The weather is really nice outside this morning.\",\n",
    "    \"This is the sixteenth and final sentence in our batch.\",\n",
    "    \"The curious traveler walked along the ancient path, carrying a notebook filled with questions about history, philosophy, language, and the endless stories of civilizations that had risen and fallen. Each step he took revealed fragments of forgotten wisdom: the ruins of temples, broken columns, faded carvings, and scattered pottery, all whispering about the lives of people who once dreamed, loved, fought, and created with astonishing determination. He reflected on how knowledge was not a fixed possession but a flowing river, always reshaping itself as generations asked new questions and found new answers, often discovering contradictions that forced them to think deeper. The traveler ...\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(tokens.keys())  # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "# print(tokens[\"input_ids\"].shape)   # [batch_size, seq_len]\n",
    "\n",
    "src_vocab = tgt_vocab = tokenizer.vocab_size  # 30522\n",
    "model = make_model(src_vocab, tgt_vocab, N=6, d_model=768, d_ff=2048, h=8, dropout=0.1, log_on=True)\n",
    "\n",
    "# print(\"模型参数数量:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "src = tokens[\"input_ids\"]  # [batch_size, seq_len]\n",
    "tgt = tokens[\"input_ids\"]  # 简单测试，目标用相同句子\n",
    "# print(src.shape, tgt.shape)\n",
    "# 注意 mask 也要传\n",
    "src_mask = tokens[\"attention_mask\"].unsqueeze(1).unsqueeze(2)  # [batch_size, 1,1,seq_len]\n",
    "# print(src_mask.shape)\n",
    "tgt_mask = None  # 测试先不做decoder mask\n",
    "\n",
    "out = model(src, tgt, src_mask, tgt_mask)\n",
    "print(\"模型输出 shape:\", out.shape)  # [batch, seq_len, tgt_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39496985",
   "metadata": {},
   "source": [
    "- `Transformer` 结构\n",
    "\n",
    "<div\n",
    "    style=\"width: 600px; background-color: white; margin: 0 auto; padding: 20px\"\n",
    ">\n",
    "    <img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" alt=\"结构\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285654fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
