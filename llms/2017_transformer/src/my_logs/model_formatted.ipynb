{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3930f370",
   "metadata": {},
   "source": [
    "```py\n",
    "EncoderDecoder.forward 输入: [torch.Size([16, 128]), torch.Size([16, 128]), torch.Size([16, 1, 1, 128])] # NT: input_ids [16, 128]\n",
    "    # HL: Inputs 模块的嵌入层 + 位置编码\n",
    "    # NOTE: 16是批次大小，16个句子，每个句子128个tokens\n",
    "    Sequential.forward 输入: [torch.Size([16, 128])] \n",
    "        Embeddings.forward 输入: [torch.Size([16, 128])]\n",
    "            Embedding.forward 输入:[torch.Size([16, 128])] 输出:torch.Size([16, 128, 768])\n",
    "        Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
    "        PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
    "            Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "        PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
    "    Sequential.forward 输出: torch.Size([16, 128, 768])\n",
    "    # HL: Encoder 模块 (input_embeddings, src_mask)\n",
    "    # NT: input_embeddings [batch_size, seq_len, embedding_dim] # 每一个 token -> 768 维的向量\n",
    "    # NT: src_mask [batch_size, 1, 1, seq_len]: 0 表示[padding]\n",
    "    Encoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "            # NT: 原始论文(Post-LN): Output = LayerNorm(Sublayer(Input) + Input)\n",
    "            # NT: 实践最优(Pre-LN): Output = Sublayer(LayerNorm(Input)) + Input\n",
    "            #                CODE: x + self.dropout(sublayer(self.norm(x)))\n",
    "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])] # HL: 多头注意力层(self_attn(x, x, x, src_mask))\n",
    "                # SOL: 所以这里先层归一化再进行计算\n",
    "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
    "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])] # HL: 前馈层\n",
    "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
    "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
    "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
    "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
    "        EncoderLayer.forward 输出: torch.Size([16, 128, 768]) * 6 # HL: 6个EncoderLayer堆叠，6次结束之后的K和V才会传递到Decoder的Encoder-Decoder Attention层\n",
    "        LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "    Encoder.forward 输出: torch.Size([16, 128, 768])\n",
    "    Sequential.forward 输入: [torch.Size([16, 128])] # HL: Outputs 模块的嵌入层+位置编码\n",
    "        Embeddings.forward 输入: [torch.Size([16, 128])]\n",
    "            Embedding.forward 输入:[torch.Size([16, 128])] 输出:torch.Size([16, 128, 768])\n",
    "        Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
    "        PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
    "            Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "        PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
    "    Sequential.forward 输出: torch.Size([16, 128, 768])\n",
    "    # HL: Decoder 模块 (output_embedding, encoder_output, tgt_mask)\n",
    "    # NT: 第二个输入来自encoder_output\n",
    "    Decoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "        # HL: 解码器的Decoder的输入(\n",
    "            # HL: 目标序列的嵌入表示[batch_size, tgt_seq_len, d_model] + \n",
    "            # HL: 编码器encoder的输出[batch_size, input_seq_len, d_model] + \n",
    "            # HL: 目标序列的mask [batch_size, 1, 1, tgt_seq_len]\n",
    "        # HL: )\n",
    "        # HL: 一个Decoder层有6个一模一样的DecoderLayer子层\n",
    "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])] # HL: 多头自注意力层 (self_attn(x, x, x, tgt_mask=None))\n",
    "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
    "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
    "            # SOL: Encoder-Decoder Attention(这个注意力层的输入包含来自Encoder的输出)\n",
    "            # NT: x decoder当前状态\n",
    "            # NT: memory 来自encoder的K\n",
    "            # NT: memory 来自encoder的V\n",
    "            # NT: src_mask 来自encoder的mask\n",
    "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])] # HL: 多头注意力层(src_attn(x, memory, memory, src_mask))\n",
    "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
    "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])] # HL: 前馈层\n",
    "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
    "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
    "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
    "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
    "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
    "        DecoderLayer.forward 输出: torch.Size([16, 128, 768]) * 6\n",
    "        LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
    "    Decoder.forward 输出: torch.Size([16, 128, 768])\n",
    "EncoderDecoder.forward 输出: torch.Size([16, 128, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afeb129",
   "metadata": {},
   "source": [
    "### 残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # 残差连接: x + dropout(sublayer(layernorm(x)))\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95253df",
   "metadata": {},
   "source": [
    "- 残差连接:\n",
    "  1. 缓解梯度消失：让梯度直接回传\n",
    "     - $y=F(x)$, 导数$d\\text{loss}/dx = d\\text{loss}/dy \\times dF(x)/dx$\n",
    "     - $y=x + F(x)$, 导数$d\\text{loss}/dx = d\\text{loss}/dy \\times (1 + dF(x)/dx)$\n",
    "     - 即使$dF(x)/dx$趋于0, 仍然保证$d\\text{loss}/dy$这部分\n",
    "     - 让100层网络的训练像10层网络一样稳定\n",
    "     - 底层参数也能获得有效的梯度更新\n",
    "  2. 保持信息流动：保留原始输入信息\n",
    "     - 每层输出 = 原始输入 + 学习到的变化, output = input + learned_change\n",
    "     - 恒等映射捷径：原始输入 x 直接传递到输出\n",
    "     - 选择性学习：网络只需要学习相对于输入的\"残差\" F(x)\n",
    "     - 信息无损传递：重要特征不会在深层中丢失\n",
    "  3. 训练稳定性：使深层网络更容易训练\n",
    "     - 深层网络容易出现的问题:\n",
    "       - Loss震荡剧烈\n",
    "       - 需要精心调参和学习率策略\n",
    "       - 对初始化敏感\n",
    "     - 训练过程更加稳定和平滑\n",
    "     - 对超参数的敏感性降低\n",
    "     - 可以使用更大的学习率加速收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20d077",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
