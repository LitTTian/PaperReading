{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee6c25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoder.forward 输入: [torch.Size([16, 128]), torch.Size([16, 128]), torch.Size([16, 1, 1, 128])]\n",
      "    Sequential.forward 输入: [torch.Size([16, 128])]\n",
      "        Embeddings.forward 输入: [torch.Size([16, 128])]\n",
      "            Embedding.forward 输入:[torch.Size([16, 128])] 输出:torch.Size([16, 128, 768])\n",
      "        Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
      "        PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
      "            Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "        PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
      "    Sequential.forward 输出: torch.Size([16, 128, 768])\n",
      "    Encoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        EncoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "    Encoder.forward 输出: torch.Size([16, 128, 768])\n",
      "    Sequential.forward 输入: [torch.Size([16, 128])]\n",
      "        Embeddings.forward 输入: [torch.Size([16, 128])]\n",
      "            Embedding.forward 输入:[torch.Size([16, 128])] 输出:torch.Size([16, 128, 768])\n",
      "        Embeddings.forward 输出: torch.Size([16, 128, 768])\n",
      "        PositionalEncoding.forward 输入: [torch.Size([16, 128, 768])]\n",
      "            Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "        PositionalEncoding.forward 输出: torch.Size([16, 128, 768])\n",
      "    Sequential.forward 输出: torch.Size([16, 128, 768])\n",
      "    Decoder.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输入: [torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 128, 768]), torch.Size([16, 1, 1, 128])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 8, 128, 128])] 输出:torch.Size([16, 8, 128, 128])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                MultiHeadedAttention.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输入: [torch.Size([16, 128, 768])]\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Dropout.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 2048])\n",
      "                    Linear.forward 输入:[torch.Size([16, 128, 2048])] 输出:torch.Size([16, 128, 768])\n",
      "                PositionwiseFeedForward.forward 输出: torch.Size([16, 128, 768])\n",
      "                Dropout.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "            SublayerConnection.forward 输出: torch.Size([16, 128, 768])\n",
      "        DecoderLayer.forward 输出: torch.Size([16, 128, 768])\n",
      "        LayerNorm.forward 输入:[torch.Size([16, 128, 768])] 输出:torch.Size([16, 128, 768])\n",
      "    Decoder.forward 输出: torch.Size([16, 128, 768])\n",
      "EncoderDecoder.forward 输出: torch.Size([16, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "def format_logs(log_lines):\n",
    "    result = []\n",
    "    stack = []  # 用来记录层级缩进\n",
    "\n",
    "    i = 0\n",
    "    while i < len(log_lines):\n",
    "        line = log_lines[i].strip()\n",
    "        if \"输入\" in line:\n",
    "            # 当前模块名，例如 \"Embedding.forward\"\n",
    "            module = line.split(\"] \")[1].split(\" 输入:\")[0]\n",
    "\n",
    "            # 检查下一行是不是对应的输出\n",
    "            if (\n",
    "                i + 1 < len(log_lines)\n",
    "                and module in log_lines[i + 1]\n",
    "                and \"输出\" in log_lines[i + 1]\n",
    "            ):\n",
    "                indent = \"    \" * len(stack)\n",
    "                inp = line.split(\"输入:\")[1].strip()\n",
    "                out = log_lines[i + 1].split(\"输出:\")[1].strip()\n",
    "                result.append(f\"{indent}[LOG] {module} 输入:{inp} 输出:{out}\")\n",
    "                i += 2\n",
    "                continue\n",
    "            else:\n",
    "                indent = \"    \" * len(stack)\n",
    "                result.append(indent + line)\n",
    "                stack.append(module)\n",
    "        elif \"输出\" in line:\n",
    "            # 输出但没有输入配对\n",
    "            indent = \"    \" * (len(stack) - 1 if stack else 0)\n",
    "            result.append(indent + line)\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "        else:\n",
    "            # 其他 log 原样缩进\n",
    "            indent = \"    \" * len(stack)\n",
    "            result.append(indent + line)\n",
    "        i += 1\n",
    "    # 删除`[LOG]`\n",
    "    result = [line.replace(\"[LOG] \", \"\") for line in result]\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"model.log\", \"r\") as f:\n",
    "        log_lines = f.readlines()\n",
    "\n",
    "    formatted = format_logs(log_lines)\n",
    "    with open(\"formatted_log.txt\", \"w\") as f:\n",
    "        f.write(formatted)\n",
    "    print(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcad452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8c317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
