```py
BertModel () input: [torch.Size([2, 8])]
  # MODULE: BertEmbeddings [batch_size, seq_length] -> [batch_size, seq_length, hidden_size]
  BertEmbeddings (embeddings) input: [torch.Size([2, 8])]
    Embedding (embeddings.word_embeddings) input: [torch.Size([2, 8])] -> output: torch.Size([2, 8, 768])
    Embedding (embeddings.position_embeddings) input: [torch.Size([1, 8])] -> output: torch.Size([1, 8, 768])
    Embedding (embeddings.token_type_embeddings) input: [torch.Size([2, 8])] -> output: torch.Size([2, 8, 768])
    LayerNorm (embeddings.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
    Dropout (embeddings.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
  BertEmbeddings (embeddings) output: torch.Size([2, 8, 768])

  BertEncoder (encoder) input: [torch.Size([2, 8, 768])]
      BertLayer (encoder.layer.0) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.0.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.0.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.0.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.0.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.0.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.0.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.0.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.0.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.0.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.0.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.0.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.0.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.0.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.0.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.0.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.0.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.0.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.0.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.0.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.0.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.0.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.0) output: <class 'tuple'>
      BertLayer (encoder.layer.1) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.1.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.1.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.1.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.1.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.1.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.1.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.1.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.1.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.1.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.1.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.1.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.1.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.1.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.1.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.1.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.1.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.1.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.1.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.1.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.1.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.1.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.1) output: <class 'tuple'>
      BertLayer (encoder.layer.2) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.2.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.2.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.2.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.2.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.2.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.2.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.2.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.2.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.2.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.2.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.2.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.2.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.2.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.2.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.2.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.2.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.2.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.2.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.2.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.2.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.2.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.2) output: <class 'tuple'>
      BertLayer (encoder.layer.3) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.3.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.3.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.3.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.3.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.3.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.3.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.3.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.3.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.3.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.3.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.3.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.3.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.3.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.3.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.3.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.3.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.3.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.3.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.3.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.3.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.3.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.3) output: <class 'tuple'>
      BertLayer (encoder.layer.4) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.4.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.4.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.4.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.4.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.4.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.4.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.4.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.4.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.4.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.4.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.4.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.4.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.4.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.4.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.4.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.4.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.4.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.4.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.4.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.4.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.4.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.4) output: <class 'tuple'>
      BertLayer (encoder.layer.5) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.5.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.5.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.5.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.5.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.5.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.5.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.5.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.5.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.5.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.5.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.5.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.5.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.5.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.5.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.5.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.5.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.5.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.5.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.5.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.5.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.5.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.5) output: <class 'tuple'>
      BertLayer (encoder.layer.6) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.6.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.6.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.6.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.6.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.6.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.6.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.6.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.6.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.6.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.6.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.6.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.6.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.6.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.6.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.6.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.6.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.6.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.6.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.6.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.6.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.6.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.6) output: <class 'tuple'>
      BertLayer (encoder.layer.7) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.7.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.7.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.7.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.7.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.7.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.7.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.7.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.7.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.7.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.7.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.7.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.7.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.7.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.7.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.7.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.7.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.7.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.7.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.7.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.7.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.7.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.7) output: <class 'tuple'>
      BertLayer (encoder.layer.8) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.8.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.8.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.8.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.8.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.8.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.8.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.8.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.8.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.8.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.8.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.8.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.8.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.8.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.8.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.8.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.8.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.8.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.8.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.8.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.8.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.8.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.8) output: <class 'tuple'>
      BertLayer (encoder.layer.9) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.9.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.9.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.9.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.9.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.9.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.9.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.9.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.9.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.9.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.9.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.9.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.9.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.9.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.9.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.9.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.9.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.9.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.9.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.9.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.9.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.9.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.9) output: <class 'tuple'>
      BertLayer (encoder.layer.10) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.10.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.10.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.10.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.10.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.10.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.10.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.10.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.10.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.10.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.10.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.10.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.10.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.10.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.10.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.10.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.10.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.10.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.10.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.10.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.10.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.10.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.10) output: <class 'tuple'>
      BertLayer (encoder.layer.11) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
        BertAttention (encoder.layer.11.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]
          BertSdpaSelfAttention (encoder.layer.11.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]
            Linear (encoder.layer.11.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.11.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Linear (encoder.layer.11.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSdpaSelfAttention (encoder.layer.11.attention.self) output: <class 'tuple'>
          BertSelfOutput (encoder.layer.11.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]
            Linear (encoder.layer.11.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            LayerNorm (encoder.layer.11.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
            Dropout (encoder.layer.11.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          BertSelfOutput (encoder.layer.11.attention.output) output: torch.Size([2, 8, 768])
        BertAttention (encoder.layer.11.attention) output: <class 'tuple'>
        BertIntermediate (encoder.layer.11.intermediate) input: [torch.Size([2, 8, 768])]
          Linear (encoder.layer.11.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])
          GELUActivation (encoder.layer.11.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])
        BertIntermediate (encoder.layer.11.intermediate) output: torch.Size([2, 8, 3072])
        BertOutput (encoder.layer.11.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]
          Linear (encoder.layer.11.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])
          LayerNorm (encoder.layer.11.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
          Dropout (encoder.layer.11.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])
        BertOutput (encoder.layer.11.output) output: torch.Size([2, 8, 768])
      BertLayer (encoder.layer.11) output: <class 'tuple'>
  BertEncoder (encoder) output: <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>
  BertPooler (pooler) input: [torch.Size([2, 8, 768])]
    Linear (pooler.dense) input: [torch.Size([2, 768])] -> output: torch.Size([2, 768])
    Tanh (pooler.activation) input: [torch.Size([2, 768])] -> output: torch.Size([2, 768])
  BertPooler (pooler) output: torch.Size([2, 768])
BertModel () output: <class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>
````