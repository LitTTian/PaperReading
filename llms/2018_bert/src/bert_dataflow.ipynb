{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d880269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddings 输入: []\n",
      "    Embedding 输入: [(1, 8)] -> 输出: (1, 8, 768)\n",
      "    Embedding 输入: (1, 8, 768) -> 输出: [(1, 8)]\n",
      "    Embedding 输入: [(1, 8)] -> 输出: (1, 8, 768)\n",
      "    Embedding 输入: (1, 8, 768) -> 输出: [(1, 8)]\n",
      "    Embedding 输入: [(1, 8)] -> 输出: (1, 8, 768)\n",
      "    Embedding 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "    LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "    LayerNorm 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "    Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "BertEmbeddings 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "    BertLayer 输入: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "        BertAttention 输入: [(1, 8, 768)]\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSdpaSelfAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768), (1, 8, 768)]\n",
      "                Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "                Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "                LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            BertSelfOutput 输出: (1, 8, 768)\n",
      "        BertAttention 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 768)] -> 输出: (1, 8, 3072)\n",
      "            Linear 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072)]\n",
      "            GELUActivation 输入: [(1, 8, 3072)] -> 输出: (1, 8, 3072)\n",
      "        BertIntermediate 输入: (1, 8, 3072) -> 输出: [(1, 8, 3072), (1, 8, 768)]\n",
      "            Linear 输入: [(1, 8, 3072)] -> 输出: (1, 8, 768)\n",
      "            Linear 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            Dropout 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "            Dropout 输入: (1, 8, 768) -> 输出: [(1, 8, 768)]\n",
      "            LayerNorm 输入: [(1, 8, 768)] -> 输出: (1, 8, 768)\n",
      "        BertOutput 输出: (1, 8, 768)\n",
      "    BertLayer 输出: [(1, 8, 768)]\n",
      "BertEncoder 输入: [(1, 8, 768)] -> 输出: [(1, 8, 768)]\n",
      "    Linear 输入: [(1, 768)] -> 输出: (1, 768)\n",
      "    Linear 输入: (1, 768) -> 输出: [(1, 768)]\n",
      "    Tanh 输入: [(1, 768)] -> 输出: (1, 768)\n",
      "BertPooler 输出: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, I love transformers!\", return_tensors=\"pt\")\n",
    "\n",
    "# 维护缩进层级\n",
    "indent_level = 0\n",
    "module_stack = []\n",
    "logs = []\n",
    "levels = []\n",
    "\n",
    "def shape_repr(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return tuple(x.shape)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [tuple(t.shape) for t in x if isinstance(t, torch.Tensor)]\n",
    "    elif isinstance(x, (BaseModelOutputWithPoolingAndCrossAttentions, BaseModelOutputWithPastAndCrossAttentions)):\n",
    "        shapes = []\n",
    "        for v in x.__dict__.values():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                shapes.append(tuple(v.shape))\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                shapes.extend([tuple(t.shape) for t in v if isinstance(t, torch.Tensor)])\n",
    "        return shapes\n",
    "    return None\n",
    "\n",
    "def pre_hook(module, inputs):\n",
    "    global indent_level\n",
    "    module_stack.append(module.__class__.__name__)\n",
    "    prefix = \"    \" * indent_level\n",
    "    in_shape = shape_repr(inputs)\n",
    "    log_str = f\"{prefix}{module.__class__.__name__} 输入: {in_shape}\"\n",
    "    # print(log_str)\n",
    "    # logs.append(log_str)\n",
    "    logs.append({\n",
    "        \"prefix\": prefix,\n",
    "        \"module\": module.__class__.__name__,\n",
    "        \"shape\": in_shape,\n",
    "        \"type\": \"输入\"\n",
    "    })\n",
    "    levels.append(indent_level)\n",
    "    indent_level += 1\n",
    "\n",
    "def post_hook(module, inputs, output):\n",
    "    global indent_level\n",
    "    indent_level -= 1\n",
    "    module_stack.pop()\n",
    "    prefix = \"    \" * indent_level\n",
    "    out_shape = shape_repr(output)\n",
    "    # print(f\"{prefix}{module.__class__.__name__} 输出: {out_shape}\")\n",
    "    log_str = f\"{prefix}{module.__class__.__name__} 输出: {out_shape}\"\n",
    "    # print(log_str)\n",
    "    logs.append({\n",
    "        \"prefix\": prefix,\n",
    "        \"module\": module.__class__.__name__,\n",
    "        \"shape\": out_shape,\n",
    "        \"type\": \"输出\"\n",
    "    })\n",
    "    levels.append(indent_level)\n",
    "\n",
    "# 给所有子模块注册 pre_hook 和 post_hook\n",
    "for name, module in model.named_modules():\n",
    "    if name != \"\":\n",
    "        module.register_forward_pre_hook(pre_hook)\n",
    "        module.register_forward_hook(post_hook)\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 合并相同模块的日志\n",
    "for i in range(0, len(logs)):\n",
    "    if i + 1 <= len(logs) - 1 and levels[i] == levels[i + 1]:\n",
    "        print(f\"{logs[i]['prefix']}{logs[i]['module']} 输入: {logs[i]['shape']} -> 输出: {logs[i + 1]['shape']}\")\n",
    "    elif i - 1 >= 0 and levels[i] == levels[i - 1]:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{logs[i]['prefix']}{logs[i]['module']} {logs[i]['type']}: {logs[i]['shape']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ffe04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "tensor([[  101,  7592,  1010,  1045,  2293, 19081,   999,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs.input_ids)\n",
    "print(inputs.token_type_ids)\n",
    "print(inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f931db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel () input: []\n",
      "  BertEmbeddings (embeddings) input: []\n",
      "    Embedding (embeddings.word_embeddings) input: [torch.Size([2, 8])] -> output: torch.Size([2, 8, 768])\n",
      "    Embedding (embeddings.position_embeddings) input: [torch.Size([1, 8])] -> output: torch.Size([1, 8, 768])\n",
      "    Embedding (embeddings.token_type_embeddings) input: [torch.Size([2, 8])] -> output: torch.Size([2, 8, 768])\n",
      "    LayerNorm (embeddings.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "    Dropout (embeddings.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "  BertEmbeddings (embeddings) output: torch.Size([2, 8, 768])\n",
      "  BertEncoder (encoder) input: [torch.Size([2, 8, 768])]\n",
      "      BertLayer (encoder.layer.0) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.0.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.0.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.0.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.0.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.0.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.0.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.0.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.0.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.0.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.0.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.0.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.0.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.0.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.0.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.0.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.0.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.0.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.0.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.0.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.0.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.0.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.0) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.1) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.1.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.1.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.1.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.1.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.1.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.1.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.1.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.1.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.1.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.1.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.1.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.1.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.1.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.1.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.1.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.1.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.1.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.1.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.1.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.1.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.1.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.1) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.2) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.2.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.2.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.2.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.2.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.2.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.2.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.2.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.2.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.2.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.2.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.2.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.2.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.2.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.2.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.2.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.2.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.2.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.2.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.2.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.2.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.2.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.2) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.3) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.3.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.3.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.3.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.3.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.3.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.3.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.3.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.3.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.3.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.3.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.3.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.3.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.3.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.3.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.3.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.3.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.3.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.3.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.3.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.3.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.3.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.3) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.4) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.4.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.4.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.4.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.4.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.4.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.4.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.4.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.4.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.4.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.4.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.4.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.4.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.4.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.4.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.4.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.4.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.4.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.4.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.4.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.4.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.4.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.4) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.5) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.5.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.5.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.5.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.5.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.5.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.5.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.5.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.5.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.5.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.5.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.5.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.5.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.5.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.5.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.5.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.5.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.5.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.5.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.5.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.5.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.5.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.5) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.6) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.6.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.6.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.6.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.6.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.6.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.6.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.6.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.6.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.6.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.6.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.6.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.6.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.6.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.6.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.6.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.6.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.6.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.6.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.6.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.6.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.6.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.6) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.7) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.7.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.7.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.7.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.7.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.7.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.7.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.7.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.7.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.7.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.7.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.7.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.7.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.7.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.7.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.7.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.7.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.7.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.7.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.7.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.7.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.7.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.7) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.8) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.8.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.8.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.8.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.8.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.8.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.8.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.8.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.8.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.8.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.8.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.8.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.8.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.8.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.8.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.8.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.8.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.8.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.8.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.8.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.8.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.8.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.8) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.9) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.9.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.9.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.9.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.9.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.9.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.9.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.9.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.9.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.9.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.9.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.9.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.9.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.9.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.9.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.9.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.9.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.9.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.9.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.9.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.9.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.9.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.9) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.10) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.10.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.10.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.10.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.10.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.10.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.10.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.10.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.10.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.10.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.10.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.10.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.10.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.10.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.10.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.10.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.10.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.10.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.10.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.10.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.10.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.10.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.10) output: <class 'tuple'>\n",
      "      BertLayer (encoder.layer.11) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "        BertAttention (encoder.layer.11.attention) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>]\n",
      "          BertSdpaSelfAttention (encoder.layer.11.attention.self) input: [torch.Size([2, 8, 768]), <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'NoneType'>, <class 'bool'>]\n",
      "            Linear (encoder.layer.11.attention.self.query) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.11.attention.self.key) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Linear (encoder.layer.11.attention.self.value) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSdpaSelfAttention (encoder.layer.11.attention.self) output: <class 'tuple'>\n",
      "          BertSelfOutput (encoder.layer.11.attention.output) input: [torch.Size([2, 8, 768]), torch.Size([2, 8, 768])]\n",
      "            Linear (encoder.layer.11.attention.output.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            LayerNorm (encoder.layer.11.attention.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "            Dropout (encoder.layer.11.attention.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          BertSelfOutput (encoder.layer.11.attention.output) output: torch.Size([2, 8, 768])\n",
      "        BertAttention (encoder.layer.11.attention) output: <class 'tuple'>\n",
      "        BertIntermediate (encoder.layer.11.intermediate) input: [torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.11.intermediate.dense) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 3072])\n",
      "          GELUActivation (encoder.layer.11.intermediate.intermediate_act_fn) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 3072])\n",
      "        BertIntermediate (encoder.layer.11.intermediate) output: torch.Size([2, 8, 3072])\n",
      "        BertOutput (encoder.layer.11.output) input: [torch.Size([2, 8, 3072]), torch.Size([2, 8, 768])]\n",
      "          Linear (encoder.layer.11.output.dense) input: [torch.Size([2, 8, 3072])] -> output: torch.Size([2, 8, 768])\n",
      "          LayerNorm (encoder.layer.11.output.LayerNorm) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "          Dropout (encoder.layer.11.output.dropout) input: [torch.Size([2, 8, 768])] -> output: torch.Size([2, 8, 768])\n",
      "        BertOutput (encoder.layer.11.output) output: torch.Size([2, 8, 768])\n",
      "      BertLayer (encoder.layer.11) output: <class 'tuple'>\n",
      "  BertEncoder (encoder) output: <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "  BertPooler (pooler) input: [torch.Size([2, 8, 768])]\n",
      "    Linear (pooler.dense) input: [torch.Size([2, 768])] -> output: torch.Size([2, 768])\n",
      "    Tanh (pooler.activation) input: [torch.Size([2, 768])] -> output: torch.Size([2, 768])\n",
      "  BertPooler (pooler) output: torch.Size([2, 768])\n",
      "BertModel () output: <class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 用于存储每个模块的输入输出和 class\n",
    "module_info = {}\n",
    "module_children = {}\n",
    "\n",
    "# 构建模块树\n",
    "def build_children_dict(module, parent_name=\"\"):\n",
    "    children = list(module.named_children())\n",
    "    module_children[parent_name] = [name for name, _ in children]\n",
    "    for name, child in children:\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        build_children_dict(child, full_name)\n",
    "\n",
    "# Hook 函数记录输入输出和 class\n",
    "def hook_fn(module, input, output, name):\n",
    "    module_info[name] = {\n",
    "        \"class\": module.__class__.__name__,\n",
    "        \"input\": [i.shape if isinstance(i, torch.Tensor) else type(i) for i in input],\n",
    "        \"output\": output.shape if isinstance(output, torch.Tensor) else type(output),\n",
    "    }\n",
    "\n",
    "# 递归打印模块输入输出\n",
    "def print_module_info(module_name, indent=0):\n",
    "    indent_str = \"  \" * indent\n",
    "    info = module_info.get(module_name)\n",
    "\n",
    "    children = module_children.get(module_name, [])\n",
    "    if info:\n",
    "        if not children:\n",
    "            print(f\"{indent_str}{info['class']} ({module_name}) input: {info['input']} -> output: {info['output']}\")\n",
    "        else:\n",
    "            # 有子模块，先打印 input\n",
    "            print(f\"{indent_str}{info['class']} ({module_name}) input: {info['input']}\")\n",
    "    for child_name in children:\n",
    "        full_child_name = f\"{module_name}.{child_name}\" if module_name else child_name\n",
    "        print_module_info(full_child_name, indent + 1)\n",
    "    if info and children:\n",
    "        print(f\"{indent_str}{info['class']} ({module_name}) output: {info['output']}\")\n",
    "\n",
    "# 初始化模型和 tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 构建模块树\n",
    "build_children_dict(model)\n",
    "\n",
    "# 注册 hook\n",
    "for name, module in model.named_modules():\n",
    "    module.register_forward_hook(lambda m, i, o, n=name: hook_fn(m, i, o, n))\n",
    "\n",
    "# 构造输入\n",
    "# text = \"Hello, this is a test.\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "sentences = [\"Hello, I love transformers!\", \"BERT is a powerful model.\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 前向传播\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "# 打印模块信息\n",
    "print_module_info(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42f3671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  1045,  2293, 19081,   999,   102],\n",
      "        [  101, 14324,  2003,  1037,  3928,  2944,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f226b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
