{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f93e2b6",
   "metadata": {},
   "source": [
    "<!-- 高亮文字 -->\n",
    "<div style=\"background-color: #ffffcc; padding: 10px; border-radius: 5px; margin-bottom: 10px;\">\n",
    "  <strong>注意：</strong> 该仓库过于老旧，可能没法适配新的Hugging face等提供的接口\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38e08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[31 51 99]\n",
      " [15  5  0]], shape=(2, 3), dtype=int32)\n",
      "Pooled output shape: (2, 512)\n",
      "Label embeddings shape: (512, 10)\n",
      "Logits shape: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "# 已经转换为 WordPiece 令牌 IDs\n",
    "input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])  # (B:2, seq_len: 3)\n",
    "print(input_ids)\n",
    "input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])  # mask: 掩码1秒时有含义，0表示pad\n",
    "token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "  num_hidden_layers=8, num_attention_heads=8, intermediate_size=1024)\n",
    "\n",
    "model = modeling.BertModel(config=config, is_training=True,\n",
    "  input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "\n",
    "num_labels = 10  # 关键：根据你的任务修改（例如二分类=2、多分类=10、NER=12等）\n",
    "initializer_range = config.initializer_range  # 复用 BERT 配置的初始化标准差（默认 0.02）\n",
    "label_embeddings = tf.Variable(\n",
    "    initial_value=tf.random.truncated_normal(\n",
    "        shape=[config.hidden_size, num_labels],  # 512 (hidden_size) × 10 (num_labels)\n",
    "        stddev=initializer_range,  # 遵循 BERT 初始化逻辑，避免权重过大\n",
    "        dtype=tf.float32\n",
    "    ),\n",
    "    name=\"label_embeddings\",  # 变量名称，便于后续调试和保存\n",
    "    trainable=True  # 训练过程中更新嵌入矩阵（关键：若固定标签嵌入可设为 False）\n",
    ")\n",
    "pooled_output = model.get_pooled_output()\n",
    "# 含义：\n",
    "# - BERT 输出有两个核心部分：\n",
    "#   a. sequence_output(T1...TN)：序列输出 → 形状 (2, 3, 512) → 每个令牌的特征向量（用于序列标注任务，如 NER）\n",
    "#   b. pooled_output(C)：池化输出 → 形状 (2, 512) → 整个文本的全局特征向量（用于分类任务）\n",
    "# - 池化逻辑：通常取第一个令牌 [CLS] 的特征向量，再经过全连接层和激活函数得到\n",
    "# - 用途：作为整个文本的“语义摘要”，用于文本分类、情感分析等任务\n",
    "logits = tf.matmul(pooled_output, label_embeddings)  # 通过线性分类器\n",
    "\n",
    "# 验证输出形状（可选）\n",
    "print(\"Pooled output shape:\", pooled_output.shape)  # 输出 (2, 512) → (batch_size=2, hidden_size=512)\n",
    "print(\"Label embeddings shape:\", label_embeddings.shape)  # 输出 (512, 10) → (hidden_size=512, num_labels=10)\n",
    "print(\"Logits shape:\", logits.shape)  # 输出 (2, 10) → (batch_size=2, num_labels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915351a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
