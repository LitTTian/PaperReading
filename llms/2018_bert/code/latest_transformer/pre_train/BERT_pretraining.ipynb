{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df78b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForPreTraining\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3afa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df82bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144f3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载数据集（wikitext-2，简化版Wikipedia）\n",
    "# from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_texts = [text for text in dataset[\"train\"][\"text\"] if len(text.strip()) > 0]  # 过滤空文本\n",
    "# print(\"\\n\".join(train_texts[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebeff511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 初始化BERT tokenizer（使用预训练的tokenizer，保证词汇表一致性）\n",
    "# from transformers import BertTokenizer, BertConfig, BertForPreTraining\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_SEQ_LENGTH = 128  # 句子最大长度（BERT-base默认512，这里简化为128）\n",
    "MASK_PROB = 0.15  # MLM任务的掩盖概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2120f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 构建NSP句子对数据集\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "class BertPretrainDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_seq_length, mask_prob):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.mask_prob = mask_prob\n",
    "        self.sentences = self._split_into_sentences()  # 拆分所有文本为句子列表\n",
    "\n",
    "    def _split_into_sentences(self):\n",
    "        \"\"\"将文本拆分为句子（简单按句号分割，实际可用spaCy等工具优化）\"\"\"\n",
    "        sentences = []\n",
    "        for text in self.texts:\n",
    "            # 分割句子并过滤空句子\n",
    "            sents = [sent.strip() for sent in text.split(\".\") if len(sent.strip()) > 5]\n",
    "            sentences.extend(sents)\n",
    "        return sentences\n",
    "\n",
    "    def _create_nsp_example(self, idx):\n",
    "        \"\"\"创建NSP任务的句子对：50%正例（连续句子），50%负例（随机句子）\"\"\"\n",
    "        # 正例：取第idx句和第idx+1句（确保不越界）\n",
    "        if random.random() < 0.5 and idx < len(self.sentences) - 1:\n",
    "            sentence1 = self.sentences[idx]\n",
    "            sentence2 = self.sentences[idx + 1]\n",
    "            is_next = 1  # 下一句标记\n",
    "        # 负例：取第idx句和随机一句（排除连续句）\n",
    "        else:\n",
    "            sentence1 = self.sentences[idx]\n",
    "            random_idx = random.choice([i for i in range(len(self.sentences)) if i != idx and i != idx + 1])\n",
    "            sentence2 = self.sentences[random_idx]\n",
    "            is_next = 0  # 非下一句标记\n",
    "        return sentence1, sentence2, is_next\n",
    "\n",
    "    def _apply_mlm(self, input_ids):\n",
    "        \"\"\"对输入token应用MLM掩码策略\"\"\"\n",
    "        input_ids = input_ids.clone()  # 避免修改原数据\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "\n",
    "        # 随机选择要掩盖的位置（排除[CLS]、[SEP]、[PAD]）\n",
    "        mask_positions = torch.bernoulli(torch.full(input_ids.shape, self.mask_prob)).bool()\n",
    "        mask_positions = mask_positions & (input_ids != self.tokenizer.cls_token_id) & (input_ids != self.tokenizer.sep_token_id) & (input_ids != self.tokenizer.pad_token_id)\n",
    "\n",
    "        for pos in torch.where(mask_positions)[0]:\n",
    "            if random.random() < 0.8:\n",
    "                # 80%替换为[MASK]\n",
    "                input_ids[pos] = self.tokenizer.mask_token_id\n",
    "            elif random.random() < 0.5:  # 剩下的20%中，50%替换为随机token\n",
    "                input_ids[pos] = random.randint(0, vocab_size - 1)\n",
    "            # 10%保持原token（不做处理）\n",
    "\n",
    "        # 生成MLM任务的标签：仅掩盖位置为原token，其他位置为-100（PyTorch忽略-100的损失）\n",
    "        mlm_labels = input_ids.clone()\n",
    "        mlm_labels[~mask_positions] = -100\n",
    "        return input_ids, mlm_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences) - 1  # 避免idx+1越界\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 生成NSP句子对\n",
    "        sentence1, sentence2, is_next = self._create_nsp_example(idx)\n",
    "\n",
    "        # 2. Tokenize句子对（BERT要求格式：[CLS] sent1 [SEP] sent2 [SEP]）\n",
    "        encoded = self.tokenizer(\n",
    "            sentence1, sentence2,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"longest_first\",\n",
    "            # truncation=\"only_first\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].flatten()  # (max_seq_length,)\n",
    "        attention_mask = encoded[\"attention_mask\"].flatten()  # (max_seq_length,)\n",
    "        token_type_ids = encoded[\"token_type_ids\"].flatten()  # (max_seq_length,)：0=sent1，1=sent2\n",
    "\n",
    "        # 3. 应用MLM掩码\n",
    "        input_ids_mlm, mlm_labels = self._apply_mlm(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_mlm,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"mlm_labels\": mlm_labels,\n",
    "            \"nsp_label\": torch.tensor(is_next, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02364dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集和数据加载器\n",
    "train_dataset = BertPretrainDataset(train_texts, tokenizer, MAX_SEQ_LENGTH, MASK_PROB)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6e373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量：67,284,284\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertConfig, BertForPreTraining\n",
    "# 配置BERT模型参数（简化版BERT-base，实际BERT-base为12层、768隐藏层、12头注意力）\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,  # 词汇表大小（bert-base-uncased为30522）\n",
    "    hidden_size=768,  # 隐藏层维度\n",
    "    num_hidden_layers=6,  # 编码器层数（简化为6层，原12层）\n",
    "    num_attention_heads=12,  # 注意力头数\n",
    "    intermediate_size=3072,  # 前馈网络隐藏层维度（768*4）\n",
    "    max_position_embeddings=MAX_SEQ_LENGTH,  # 最大序列长度\n",
    "    type_vocab_size=2,  # token_type_ids的类别数（0和1）\n",
    "    hidden_dropout_prob=0.1,  # Dropout概率\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "\n",
    "# 初始化预训练模型（BertForPreTraining已包含MLM和NSP头）\n",
    "model = BertForPreTraining(config)\n",
    "print(f\"模型参数量：{sum(p.numel() for p in model.parameters()):,}\")  # 约8000万参数（简化版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12400a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 优化器：AdamW（权重衰减防止过拟合）\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,  # BERT预训练常用学习率\n",
    "    weight_decay=0.01  # 权重衰减\n",
    ")\n",
    "\n",
    "# 训练参数\n",
    "EPOCHS = 1  # 预训练轮数（实际需10+轮，这里简化为3轮）\n",
    "accumulation_steps = 4  # 梯度累积（模拟更大batch_size，如32*4=128）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026b3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2788/2788 [07:01<00:00,  6.61it/s, Total Loss=1.0720, MLM Loss=0.3783, NSP Loss=0.6937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 1.7253\n",
      "预训练完成，模型已保存到 ./bert_pretrained_simple\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # 1. 数据移到GPU/CPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        mlm_labels = batch[\"mlm_labels\"].to(device)\n",
    "        nsp_label = batch[\"nsp_label\"].to(device)\n",
    "\n",
    "        # 2. 前向传播：模型输出MLM logits和NSP logits\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,  # MLM任务标签\n",
    "            next_sentence_label=nsp_label  # NSP任务标签\n",
    "        )\n",
    "        # 旧版outputs包含：total_loss, mlm_loss, nsp_loss, mlm_logits, nsp_logits\n",
    "        total_loss = outputs.loss\n",
    "        mlm_logits = outputs.prediction_logits  # (batch_size, seq_len, vocab_size)\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            mlm_logits.reshape(-1, tokenizer.vocab_size),  # (batch*seq_len, vocab_size)\n",
    "            mlm_labels.reshape(-1),  # (batch*seq_len,)\n",
    "            ignore_index=-100  # 忽略非掩码位置\n",
    "        )\n",
    "        nsp_logits = outputs.seq_relationship_logits  # (batch_size, 2)\n",
    "        nsp_loss = F.cross_entropy(nsp_logits, nsp_label)\n",
    "\n",
    "        # 3. 反向传播（梯度累积）\n",
    "        total_loss = total_loss / accumulation_steps  # 累积步长归一化\n",
    "        total_loss.backward()\n",
    "\n",
    "        # 4. 梯度更新（每accumulation_steps步更新一次）\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_step += 1\n",
    "\n",
    "        # 5. 统计损失\n",
    "        epoch_loss += total_loss.item() * accumulation_steps  # 还原真实损失\n",
    "        progress_bar.set_postfix({\n",
    "            \"Total Loss\": f\"{total_loss.item()*accumulation_steps:.4f}\",\n",
    "            \"MLM Loss\": f\"{mlm_loss.item():.4f}\",\n",
    "            \"NSP Loss\": f\"{nsp_loss.item():.4f}\"\n",
    "        })\n",
    "\n",
    "    # 打印每轮平均损失\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# 保存预训练模型（供下游任务微调使用）\n",
    "model.save_pretrained(\"./bert_pretrained_simple\")\n",
    "tokenizer.save_pretrained(\"./bert_pretrained_simple\")\n",
    "print(\"预训练完成，模型已保存到 ./bert_pretrained_simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578cd5b",
   "metadata": {},
   "source": [
    "## TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4e76c",
   "metadata": {},
   "source": [
    "#### TEST my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e29f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原句子：I [MASK] a book yesterday.\n",
      "预测结果：I [MASK] a book yesterday.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_sentence = \"I [MASK] a book yesterday.\"\n",
    "encoded = tokenizer(test_sentence, return_tensors=\"pt\").to(device)\n",
    "# 2. 前向传播预测[MASK]（无需计算损失）\n",
    "with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "    outputs = model(**encoded)\n",
    "    mlm_logits = outputs.prediction_logits  # (1, seq_len, vocab_size)\n",
    "\n",
    "    # 3. 找到[MASK]的位置\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_positions = torch.where(encoded[\"input_ids\"][0] == mask_token_id)[0]\n",
    "    \n",
    "    # 确保找到至少一个[MASK]\n",
    "    if len(mask_positions) == 0:\n",
    "        print(\"未找到[MASK]标记！\")\n",
    "    else:\n",
    "        mask_pos = mask_positions[0]  # 取第一个[MASK]的位置\n",
    "        # 4. 预测[MASK]位置的token（取logits最大值）\n",
    "        predicted_token_id = mlm_logits[0, mask_pos].argmax(dim=-1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "        \n",
    "        # 5. 输出结果\n",
    "        print(f\"原句子：{test_sentence}\")\n",
    "        print(f\"预测结果：I {predicted_token} a book yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68610efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子对1：I like playing basketball. | It is my favorite sport. → 连贯\n",
      "句子对2：I like playing basketball. | The sky is blue today. → 连贯\n"
     ]
    }
   ],
   "source": [
    "test_sent1 = \"I like playing basketball.\"\n",
    "test_sent2_pos = \"It is my favorite sport.\"  # 正例\n",
    "test_sent2_neg = \"The sky is blue today.\"  # 负例\n",
    "\n",
    "# 正例预测\n",
    "encoded_pos = tokenizer(test_sent1, test_sent2_pos, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs_pos = model(**encoded_pos)\n",
    "    nsp_logits_pos = outputs_pos.seq_relationship_logits\n",
    "    nsp_pred_pos = nsp_logits_pos.argmax(dim=-1).item()\n",
    "print(f\"句子对1：{test_sent1} | {test_sent2_pos} → {'连贯' if nsp_pred_pos == 1 else '不连贯'}\")\n",
    "\n",
    "# 负例预测\n",
    "encoded_neg = tokenizer(test_sent1, test_sent2_neg, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs_neg = model(**encoded_neg)\n",
    "    nsp_logits_neg = outputs_neg.seq_relationship_logits\n",
    "    nsp_pred_neg = nsp_logits_neg.argmax(dim=-1).item()\n",
    "print(f\"句子对2：{test_sent1} | {test_sent2_neg} → {'连贯' if nsp_pred_neg == 1 else '不连贯'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdca7c",
   "metadata": {},
   "source": [
    "#### Test Official Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9944939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68929c29e0cc46b99e5bc8b877d43517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载官方预训练模型和Tokenizer\n",
    "model_name = \"bert-base-uncased\"  # 官方预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 按需加载不同任务的模型（官方模型已包含所有预训练权重）\n",
    "# 1. 同时支持MLM+NSP的模型（BertForPreTraining）\n",
    "model_pretrain = BertForPreTraining.from_pretrained(model_name).to(device)\n",
    "# 2. 仅MLM的轻量模型（BertForMaskedLM，推荐用于掩码预测）\n",
    "model_mlm = BertForMaskedLM.from_pretrained(model_name).to(device)\n",
    "# 3. 仅NSP的模型（BertForNextSentencePrediction）\n",
    "model_nsp = BertForNextSentencePrediction.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666f210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原句子：I [MASK] a book yesterday.\n",
      "预测结果：I read a book yesterday.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "    outputs = model_pretrain(**encoded)\n",
    "    mlm_logits = outputs.prediction_logits  # (1, seq_len, vocab_size)\n",
    "\n",
    "    # 3. 找到[MASK]的位置\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_positions = torch.where(encoded[\"input_ids\"][0] == mask_token_id)[0]\n",
    "    \n",
    "    # 确保找到至少一个[MASK]\n",
    "    if len(mask_positions) == 0:\n",
    "        print(\"未找到[MASK]标记！\")\n",
    "    else:\n",
    "        mask_pos = mask_positions[0]  # 取第一个[MASK]的位置\n",
    "        # 4. 预测[MASK]位置的token（取logits最大值）\n",
    "        predicted_token_id = mlm_logits[0, mask_pos].argmax(dim=-1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "        \n",
    "        # 5. 输出结果\n",
    "        print(f\"原句子：{test_sentence}\")\n",
    "        print(f\"预测结果：I {predicted_token} a book yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6c94c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子对1：I like playing basketball. | It is my favorite sport. → 不连贯\n",
      "句子对2：I like playing basketball. | The sky is blue today. → 不连贯\n"
     ]
    }
   ],
   "source": [
    "test_sent1 = \"I like playing basketball.\"\n",
    "test_sent2_pos = \"It is my favorite sport.\"  # 正例\n",
    "test_sent2_neg = \"The sky is blue today.\"  # 负例\n",
    "\n",
    "# 正例预测\n",
    "encoded_pos = tokenizer(test_sent1, test_sent2_pos, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs_pos = model_pretrain(**encoded_pos)\n",
    "    nsp_logits_pos = outputs_pos.seq_relationship_logits\n",
    "    nsp_pred_pos = nsp_logits_pos.argmax(dim=-1).item()\n",
    "print(f\"句子对1：{test_sent1} | {test_sent2_pos} → {'连贯' if nsp_pred_pos == 1 else '不连贯'}\")\n",
    "\n",
    "# 负例预测\n",
    "encoded_neg = tokenizer(test_sent1, test_sent2_neg, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs_neg = model_pretrain(**encoded_neg)\n",
    "    nsp_logits_neg = outputs_neg.seq_relationship_logits\n",
    "    nsp_pred_neg = nsp_logits_neg.argmax(dim=-1).item()\n",
    "print(f\"句子对2：{test_sent1} | {test_sent2_neg} → {'连贯' if nsp_pred_neg == 1 else '不连贯'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
