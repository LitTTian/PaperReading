{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于模型的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WSDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_img_and_mask\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.utils'"
     ]
    }
   ],
   "source": [
    "from utils.data_loading import WSDataset\n",
    "from unet import UNet\n",
    "from utils.utils import plot_img_and_mask\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "\n",
    "def predict_img(net,\n",
    "                full_img,\n",
    "                device,\n",
    "                scale_factor=1,\n",
    "                out_threshold=0.5):\n",
    "    net.eval()\n",
    "    new_size = (full_img.size[0] * scale_factor, full_img.size[1] * scale_factor)\n",
    "    im_transform = transforms.Compose([\n",
    "        ImageResize(new_size, interpolate_mode=Image.BICUBIC), \n",
    "        ImageNormalization(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = im_transform(full_img).unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "    print(img.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img).cpu()\n",
    "        output = F.interpolate(output, (full_img.size[1], full_img.size[0]), mode='bilinear')\n",
    "        mask = output.argmax(dim=1)\n",
    "    return mask[0].long().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoints/checkpoint_epoch7.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m net \u001b[38;5;241m=\u001b[39m UNet(n_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, bilinear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m mask_values \u001b[38;5;241m=\u001b[39m state_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_values\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/comp9517_python310/lib/python3.11/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/comp9517_python310/lib/python3.11/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/comp9517_python310/lib/python3.11/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/checkpoint_epoch7.pth'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "checkpoint_path = './checkpoints/checkpoint_epoch7.pth'\n",
    "net = UNet(n_channels=3, n_classes=16, bilinear=False)\n",
    "net.to(device=device)\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "mask_values = state_dict.pop('mask_values', [0, 1])\n",
    "net.load_state_dict(state_dict)\n",
    "test_dir = Path('.') / '..' / '..' / '..' / 'data' / 'test1'\n",
    "\n",
    "# test_input_dir = test_dir / 'image'\n",
    "# test_gt_dir = test_dir / 'indexLabel'\n",
    "# inputs = test_input_dir.glob('*.png')\n",
    "data_dir = Path('.') / '..' / '..' / 'data'\n",
    "val_csv = pd.read_csv(data_dir / 'val.csv')\n",
    "val_image_paths = val_csv['im_path'].values\n",
    "val_label_paths = val_csv['label_path'].values\n",
    "\n",
    "i = 0\n",
    "\n",
    "# print(list(inputs))\n",
    "for i in range(len(val_image_paths)):\n",
    "    # gt_path = test_gt_dir / input_path.name\n",
    "    gt_path = data_dir / val_label_paths[i]\n",
    "    gt = Image.open(gt_path)\n",
    "    image = Image.open(data_dir / val_image_paths[i])\n",
    "    mask = predict_img(net=net,\n",
    "                       full_img=image,\n",
    "                       scale_factor=1,\n",
    "                       out_threshold=0.5,\n",
    "                       device=device)\n",
    "    # print('mask true的类', np.unique(np.array(gt)))\n",
    "    # print('预测的mask的类', np.unique(np.array(mask)))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(gt, cmap='gray')\n",
    "    plt.show()\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  5  6  7  8  9 11 14 17 18]\n"
     ]
    }
   ],
   "source": [
    "gt = '../../../data/test1/indexLabel/1624325291-972695058.png'\n",
    "array = np.array(Image.open(gt))\n",
    "print(np.unique(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 7, 8, 10, 11, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "mask_true = [ 2, 5, 6, 7, 8, 9, 11, 14, 17, 18]\n",
    "mapping = [0, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 0, 11, 12, 13, 14, 15] # 类别映射\n",
    "transformed_mask = [0] * len(mask_true)\n",
    "for i, v in enumerate(mask_true):\n",
    "    transformed_mask[i] = mapping[v]\n",
    "print(transformed_mask)\n",
    "# pred = [ 1  6  7 14 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取 Mask 中的所有类值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的mask中的类 [ 2  7  8 15 17 18] ['dirt', 'tree-trunk', 'tree-foliage', 'log', 'sky', 'grass']\n",
      "转换后mask中的类 [ 1  6  7 12 14 15]\n",
      "{1: 'asphalt', 2: 'dirt', 3: 'mud', 4: 'water', 5: 'gravel', 6: 'other-terrain', 7: 'tree-trunk', 8: 'tree-foliage', 9: 'bush', 10: 'fence', 11: 'structure', 12: 'pole', 13: 'vehicle', 14: 'rock', 15: 'log', 16: 'other-object', 17: 'sky', 18: 'grass'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "def unique_values_in_mask(mask):\n",
    "    return np.unique(mask)\n",
    "def apply_mapping(mask, mapping): # Define function to apply mapping to mask\n",
    "    new_mask = np.copy(mask)\n",
    "    for old_value, new_value in enumerate(mapping):\n",
    "        new_mask[mask == old_value] = new_value\n",
    "    return new_mask\n",
    "\n",
    "mask_path = '../../data/processed/val/indexLabel/1624327528-048416193.png'\n",
    "origin_classes =[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "class_mapping = [0, 5, 1, 2, 3, 4, 5, 6, 7, 8,  9, 10, 13,  0, 11, 12, 13, 14, 15]\n",
    "classes = {'asphalt': 1, 'dirt': 2, 'mud': 3, 'water': 4, 'gravel'\n",
    ": 5, 'other-terrain': 6, 'tree-trunk': 7, 'tree-foliage': 8, 'bush': 9, 'fence': 10, 'structure': 11, 'pole': 12, 'vehicle': 13, 'rock': 14, 'log': 15, 'other-object': 16, 'sky': 17, 'grass': 18}\n",
    "inverse_dict = {1: 'asphalt', 2: 'dirt', 3: 'mud', 4: 'water', 5: 'gravel', 6: 'other-terrain', 7: 'tree-trunk', 8: 'tree-foliage', 9: 'bush', 10: 'fence', 11: 'structure', 12: 'pole', 13: 'vehicle', 14: 'rock', 15: 'log', 16: 'other-object', 17: 'sky', 18: 'grass'}\n",
    "mask = np.array(Image.open(mask_path))\n",
    "unique_values = unique_values_in_mask(mask)\n",
    "classes_in_mask = [inverse_dict[i] for i in unique_values]\n",
    "print('原始的mask中的类', unique_values, classes_in_mask)\n",
    "mapped_mask = apply_mapping(mask, class_mapping)\n",
    "print('转换后mask中的类', unique_values_in_mask(mapped_mask))\n",
    "print(inverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {5: ['asphalt', 'other-terrain'], 1: ['dirt'], 2: ['mud'], 3: ['water'], 4: ['gravel'], 6: ['tree-trunk'], 7: ['tree-foliage'], 8: ['bush'], 9: ['fence'], 10: ['structure'], 13: ['pole', 'other-object'], 0: ['vehicle', 'bg'], 11: ['rock'], 12: ['log'], 14: ['sky'], 15: ['grass']})\n",
      "0 ['vehicle', 'bg']\n",
      "1 ['dirt']\n",
      "2 ['mud']\n",
      "3 ['water']\n",
      "4 ['gravel']\n",
      "5 ['asphalt', 'other-terrain']\n",
      "6 ['tree-trunk']\n",
      "7 ['tree-foliage']\n",
      "8 ['bush']\n",
      "9 ['fence']\n",
      "10 ['structure']\n",
      "11 ['rock']\n",
      "12 ['log']\n",
      "13 ['pole', 'other-object']\n",
      "14 ['sky']\n",
      "15 ['grass']\n"
     ]
    }
   ],
   "source": [
    "final_dict = {k: class_mapping[v] for k, v in classes.items()}\n",
    "# print(final_dict)\n",
    "final_dict['bg'] = 0\n",
    "from collections import defaultdict\n",
    "inverse_final_dict = defaultdict(list)\n",
    "for k in final_dict:\n",
    "    inverse_final_dict[final_dict[k]].append(k)\n",
    "print(inverse_final_dict)\n",
    "# list1 = []\n",
    "for k in sorted(inverse_final_dict.keys()):\n",
    "    print(k, inverse_final_dict[k])\n",
    "    # list1.append(inverse_final_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 ['vehicle', 'bg']\n",
    "1 ['dirt']\n",
    "2 ['mud']\n",
    "3 ['water']\n",
    "4 ['gravel']\n",
    "5 ['asphalt', 'other-terrain']\n",
    "6 ['tree-trunk']\n",
    "7 ['tree-foliage']\n",
    "8 ['bush']\n",
    "9 ['fence']\n",
    "10 ['structure']\n",
    "11 ['rock']\n",
    "12 ['log']\n",
    "13 ['pole', 'other-object']\n",
    "14 ['sky']\n",
    "15 ['grass']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicle', 'background'] nan\n",
      "['dirt'] 0.40439\n",
      "['mud'] 0.0\n",
      "['water'] nan\n",
      "['gravel'] 0.0\n",
      "['asphalt', 'other-terrain'] 0.0\n",
      "['tree-trunk'] 0.37265\n",
      "['tree-foliage'] 0.63374\n",
      "['bush'] 1.8124e-06\n",
      "['fence'] nan\n",
      "['structure'] 0.00017964\n",
      "['rock'] 0.0\n",
      "['log'] 0.0\n",
      "['pole', 'other-object'] 0.00011568\n",
      "['sky'] 0.69367\n",
      "['grass'] 0.2816\n"
     ]
    }
   ],
   "source": [
    "nan = np.nan\n",
    "i_iou = [       nan, 4.0439e-01, 0.0000e+00,        nan, 0.0000e+00, 0.0000e+00,\n",
    "        3.7265e-01, 6.3374e-01, 1.8124e-06,        nan, 1.7964e-04, 0.0000e+00,\n",
    "        0.0000e+00, 1.1568e-04, 6.9367e-01, 2.8160e-01]\n",
    "classes = [['vehicle', 'background'], ['dirt'], ['mud'], ['water'], ['gravel'], ['asphalt', 'other-terrain'], ['tree-trunk'], ['tree-foliage'], ['bush'], ['fence'], ['structure'], ['rock'], ['log'], ['pole', 'other-object'], ['sky'], ['grass']]\n",
    "for i in range(len(i_iou)):\n",
    "    print(classes[i], i_iou[i])\n",
    "\n",
    "# 0: vehicle, 2.3 * 10^5\n",
    "# 1: dirt, 2.5 * 10^9\n",
    "# 2: mud, 1.5 * 10^7\n",
    "# 3: water, 5.8 * 10^7\n",
    "# 4: gravel, 1.1 * 10^8\n",
    "# 5: asphalt/other-terrain, 2.4*10^5+4.4*10^6 = 4.64*10^6\n",
    "# 6: tree-trunk, 3.9 * 10^9\n",
    "# 7: tree-foliage, 1.6 * 10^10\n",
    "# 8: bush, 3.4 * 10^8\n",
    "# 9: fence, 1 * 10^7\n",
    "# 10: structure, 9.8 * 10^7\n",
    "# 11: rock, 1.6*10^7\n",
    "# 12: log, 1.2* 10^8\n",
    "# 13: pole/other-object, 3.5*10^6 + 2.8 * 10^7 = 3.15 * 10^7\n",
    "# 14: sky, 2.0 * 10^9\n",
    "# 15: grass, 3 * 10^9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1624327528-048416193.png\n",
    "# classes value in mask_pred [ 1  4  6  7 10 14 15] # 4: gravel, 10: structure\n",
    "# classes value in gt.       [ 1  6  7 12 14 15] # 12: log\n",
    "\n",
    "\n",
    "# 1624327505-807209646.png\n",
    "# classes value in mask_pred [ 1  6  7 10 13 14 15] 13: other-object\n",
    "# classes value in gt.       [ 1  6  7  8 10 14 15] 8: bush\n",
    "\n",
    "\n",
    "# classes value in mask_pred [ 1  6  7 10 14 15] 10: structure\n",
    "# classes value in gt.       [ 1  6  7  8 14 15] 8: bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 类别像素点个数\n",
    "pixel_counts = [2.3e5, 2.5e9, 1.5e7, 5.8e7, 1.1e8, 4.64e6, 3.9e9, 1.6e10, 3.4e8, 1e7, 9.8e7, 1.6e7, 1.2e8, 3.15e7, 2e9, 3e9]\n",
    "pixel_counts = pixel_counts[1:]\n",
    "# 计算权重\n",
    "weights = 1 / torch.tensor(pixel_counts)\n",
    "weights = weights / weights.sum()  # 标准化，使得权重和为1\n",
    "weights = torch.cat((torch.tensor([0.0]), weights))\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "x = torch.tensor([[1, 2, 3, 4, 5],\n",
    "              [6, 7, 8, 9, 10],\n",
    "              [11, 12, 13, 14, 15],\n",
    "              [16, 17, 18, 19, 20],\n",
    "              [21, 22, 23, 24, 25]])\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517_python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
