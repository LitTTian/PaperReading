{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd37af31",
   "metadata": {},
   "source": [
    "## 引言 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f6372",
   "metadata": {},
   "source": [
    "1. 背景与现状\n",
    "医学图像分割主流技术：卷积神经网络（CNN），尤其是全卷积网络（FCN）衍生的 U-Net，凭借对称编码器 - 解码器结构 + 跳跃连接的设计，成为医学图像分割的 “默认选择”，已在 MR 心脏分割、CT 器官分割、结肠镜息肉分割等场景中取得成功。\n",
    "CNN 的局限性：卷积操作的 “局部性” 导致其难以建模长距离依赖关系，面对患者间纹理、形状、大小差异大的目标结构时，分割性能较弱。\n",
    "Transformer 的崛起：作为完全依赖注意力机制的架构，Transformer 擅长建模全局上下文，且大规模预训练后迁移能力强，已在 NLP、图像识别任务中达到 / 超越 SOTA，但尚未被系统应用于医学图像分割。\n",
    "2. 关键问题发现\n",
    "直接将 Transformer 应用于医学图像分割（图像块 tokenization 编码 + 直接上采样至全分辨率）效果不佳。\n",
    "根源：Transformer 将输入视为 1D 序列，全程聚焦全局上下文，导致特征分辨率低、缺乏精细定位信息，且直接上采样无法恢复这些信息，最终分割结果粗糙。\n",
    "3. 解决方案：TransUNet\n",
    "核心设计：首个融合 CNN 与 Transformer 的医学图像分割框架，兼顾两者优势：\n",
    "利用 CNN 提取高分辨率、含精细空间细节的低级视觉特征；\n",
    "利用 Transformer 编码全局上下文信息；\n",
    "借鉴 U-Net 的 U 形结构，将 Transformer 编码的自注意力特征上采样后，与编码器路径中跳跃连接的高分辨率 CNN 特征融合，实现精确定位。\n",
    "4. 实验结论\n",
    "TransUNet 比基于 CNN 的自注意力方法更高效地利用自注意力机制；\n",
    "深入融合低级特征能进一步提升分割精度；\n",
    "在多种医学图像分割任务中，性能优于现有竞争方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee5c5e",
   "metadata": {},
   "source": [
    "## 方法论 Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48c734",
   "metadata": {},
   "source": [
    "### 用Transformer做编码器 Transformer as Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239adcff",
   "metadata": {},
   "source": [
    "- **图像序列化 Image Sequentialization**: 把 2D 医学图像（如 CT/MR 切片）转 能处理的 1D 序列（类似 换成 TransformerNLP 中把句子拆成单词）。具体操作：\n",
    "  1. 输入图像：原始图像为$x$，尺寸为$H \\times W \\times C$。\n",
    "  2. 分割图像块：将$x$按固定大小$P \\times P$(如16x16)分割成不重叠的2D图像块。(patch)\n",
    "  3. 扁平化：每个$P \\times P \\times C$的图像块被展平为一个长度为$P^2 \\cdot C$的向量。（比如16x16x3的图像块展平为长度768的向量）\n",
    "  4. 形成序列：所有扁平化的图像块组成一个序列$\\{x_p^i\\}$，序列长度为$N = \\frac{H \\times W}{P^2}$。\n",
    "- **块嵌入 Patch Embedding**: \n",
    "  1. 线性映射：使用一个可学习的线性投影矩阵$E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$，将每个图像块向量$x_p^i$映射为$D$维嵌入向量$x_p^i E$。\n",
    "  2. 位置编码：为了保留图像块的空间位置信息，添加可学习的位置编码$E_\\text{pos} \\in \\mathbb{R}^{N \\times D}$到嵌入向量中，得到最终的输入序列$z_0 = [x_p^1 E; x_p^2 E; ...; x_p^N E] + E_\\text{pos} \\in \\mathbb{R}^{N \\times D}$。\n",
    "- **Transformer 编码器 Transformer Encoder**: Transformer 编码器由$L$层(多头自注意力层 MSA + 多层感知机 MLP)组成：\n",
    "  - $z_\\ell^\\prime = \\text{MSA}(\\text{LN}(z_{\\ell-1})) + z_{\\ell-1}$\n",
    "  - $z_\\ell = \\text{MLP}(\\text{LN}(z_\\ell^\\prime)) + z_\\ell^\\prime$\n",
    "\n",
    "- 为了做语义分割，首先将$z_L \\in \\mathbb{R}^{N \\times D}$重塑为$(\\frac{H}{P}, \\frac{W}{P}, D)$的张量特征图，然后使用$1\\times1$卷积将通道数从$D$转换为$\\#\\text{cls}$，最后直接通过双线性插值上采样(blinearly up-sample)至原始图像尺寸$(\\#\\text{cls}, H, W)$。(decoder部分被称为\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76c7b6",
   "metadata": {},
   "source": [
    "### TransUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170efa41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-radius: 5px; margin: auto; width:80%; text-align: center;\">\n",
    "    <image src=\"./assets/transunet_v2.png\" />\n",
    "    <span style=\"font-size: 12px; color: gray;\">图1 TransUNet架构示意图</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8badc7",
   "metadata": {},
   "source": [
    "- **CNN-Transformer 混合编码器 CNN-Transformer Hybrid as Encoder**: 比起上面直接用 Transformer 作为编码器（直接在原始图像上做 patch embedding），作者提出用 CNN 提取低级视觉特征，再把$1 \\times 1$块嵌入后通过Transformer。原因：\n",
    "    1. 可以在解码器路径中利用中间层的高分辨率CNN特征图；\n",
    "    2. 混合CNN-Transformer编码器比单纯的Transformer编码器更好。\n",
    "- **级联上采样器 Cascaded Upsampler**: 引入一种集联上采样器(CUP)\n",
    "  - 在将隐藏特征$z_L \\in \\mathbb{R}^{\\frac{HW}{P^2} \\times D}$重塑为$\\frac{H}{P} \\times \\frac{W}{P} \\times D$，我们通过CUP模块逐步上采样至原始分辨率$H \\times W$。\n",
    "  - 其中每个CUP模块包含2个上采样算子，1个$3 \\times 3$卷积层和1个ReLU层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ebf3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义上采样层\n",
    "in_channels = 512\n",
    "bilinear = False\n",
    "\n",
    "if bilinear:  # 使用双线性插值上采样\n",
    "    # NT: bilinear通过周围4个像素的加权平均值计算新像素值，align_corners保持角点对齐\n",
    "    up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "else:\n",
    "    # NT: 转置卷积(反卷积)，能更好保留细节，但参数更多且可能过拟合\n",
    "    up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "# 构造输入张量 (B=2, C=512, H=16, W=16)\n",
    "x = torch.randn(2, in_channels, 16, 16)\n",
    "\n",
    "# 前向传播\n",
    "out = up(x)\n",
    "\n",
    "print(out.shape)  # 输出: torch.Size([2, 512, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "        DoubleConv-7         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-8         [-1, 64, 128, 128]               0\n",
      "            Conv2d-9        [-1, 128, 128, 128]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 128, 128]             256\n",
      "             ReLU-11        [-1, 128, 128, 128]               0\n",
      "           Conv2d-12        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-13        [-1, 128, 128, 128]             256\n",
      "             ReLU-14        [-1, 128, 128, 128]               0\n",
      "       DoubleConv-15        [-1, 128, 128, 128]               0\n",
      "             Down-16        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-17          [-1, 128, 64, 64]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-19          [-1, 256, 64, 64]             512\n",
      "             ReLU-20          [-1, 256, 64, 64]               0\n",
      "           Conv2d-21          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 64, 64]             512\n",
      "             ReLU-23          [-1, 256, 64, 64]               0\n",
      "       DoubleConv-24          [-1, 256, 64, 64]               0\n",
      "             Down-25          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-26          [-1, 256, 32, 32]               0\n",
      "           Conv2d-27          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-29          [-1, 512, 32, 32]               0\n",
      "           Conv2d-30          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-31          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-32          [-1, 512, 32, 32]               0\n",
      "       DoubleConv-33          [-1, 512, 32, 32]               0\n",
      "             Down-34          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-35          [-1, 512, 16, 16]               0\n",
      "           Conv2d-36         [-1, 1024, 16, 16]       4,719,616\n",
      "      BatchNorm2d-37         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-38         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-39         [-1, 1024, 16, 16]       9,438,208\n",
      "      BatchNorm2d-40         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-41         [-1, 1024, 16, 16]               0\n",
      "       DoubleConv-42         [-1, 1024, 16, 16]               0\n",
      "             Down-43         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-44          [-1, 512, 32, 32]       2,097,664\n",
      "           Conv2d-45          [-1, 512, 32, 32]       4,719,104\n",
      "      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-47          [-1, 512, 32, 32]               0\n",
      "           Conv2d-48          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-49          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-50          [-1, 512, 32, 32]               0\n",
      "       DoubleConv-51          [-1, 512, 32, 32]               0\n",
      "               Up-52          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-53          [-1, 256, 64, 64]         524,544\n",
      "           Conv2d-54          [-1, 256, 64, 64]       1,179,904\n",
      "      BatchNorm2d-55          [-1, 256, 64, 64]             512\n",
      "             ReLU-56          [-1, 256, 64, 64]               0\n",
      "           Conv2d-57          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-58          [-1, 256, 64, 64]             512\n",
      "             ReLU-59          [-1, 256, 64, 64]               0\n",
      "       DoubleConv-60          [-1, 256, 64, 64]               0\n",
      "               Up-61          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 128, 128]         131,200\n",
      "           Conv2d-63        [-1, 128, 128, 128]         295,040\n",
      "      BatchNorm2d-64        [-1, 128, 128, 128]             256\n",
      "             ReLU-65        [-1, 128, 128, 128]               0\n",
      "           Conv2d-66        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-67        [-1, 128, 128, 128]             256\n",
      "             ReLU-68        [-1, 128, 128, 128]               0\n",
      "       DoubleConv-69        [-1, 128, 128, 128]               0\n",
      "               Up-70        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-71         [-1, 64, 256, 256]          32,832\n",
      "           Conv2d-72         [-1, 64, 256, 256]          73,792\n",
      "      BatchNorm2d-73         [-1, 64, 256, 256]             128\n",
      "             ReLU-74         [-1, 64, 256, 256]               0\n",
      "           Conv2d-75         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-76         [-1, 64, 256, 256]             128\n",
      "             ReLU-77         [-1, 64, 256, 256]               0\n",
      "       DoubleConv-78         [-1, 64, 256, 256]               0\n",
      "               Up-79         [-1, 64, 256, 256]               0\n",
      "           Conv2d-80          [-1, 1, 256, 256]              65\n",
      "          OutConv-81          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 31,043,521\n",
      "Trainable params: 31,043,521\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1020.00\n",
      "Params size (MB): 118.42\n",
      "Estimated Total Size (MB): 1139.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.model.unet import UNet\n",
    "from torchsummary import summary\n",
    "\n",
    "unet_model = UNet(n_channels=3, n_classes=1)\n",
    "summary(unet_model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f35799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             864\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "              ReLU-3         [-1, 32, 224, 224]               0\n",
      "            Conv2d-4         [-1, 32, 224, 224]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 224, 224]              64\n",
      "              ReLU-6         [-1, 32, 224, 224]               0\n",
      "        DoubleConv-7         [-1, 32, 224, 224]               0\n",
      "         MaxPool2d-8         [-1, 32, 112, 112]               0\n",
      "            Conv2d-9         [-1, 64, 112, 112]          18,432\n",
      "      BatchNorm2d-10         [-1, 64, 112, 112]             128\n",
      "             ReLU-11         [-1, 64, 112, 112]               0\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "       DoubleConv-15         [-1, 64, 112, 112]               0\n",
      "             Down-16         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-17           [-1, 64, 56, 56]               0\n",
      "           Conv2d-18          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-19          [-1, 128, 56, 56]             256\n",
      "             ReLU-20          [-1, 128, 56, 56]               0\n",
      "           Conv2d-21          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-22          [-1, 128, 56, 56]             256\n",
      "             ReLU-23          [-1, 128, 56, 56]               0\n",
      "       DoubleConv-24          [-1, 128, 56, 56]               0\n",
      "             Down-25          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-28          [-1, 256, 28, 28]             512\n",
      "             ReLU-29          [-1, 256, 28, 28]               0\n",
      "           Conv2d-30          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-31          [-1, 256, 28, 28]             512\n",
      "             ReLU-32          [-1, 256, 28, 28]               0\n",
      "       DoubleConv-33          [-1, 256, 28, 28]               0\n",
      "             Down-34          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-35          [-1, 256, 14, 14]               0\n",
      "           Conv2d-36          [-1, 512, 14, 14]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-38          [-1, 512, 14, 14]               0\n",
      "           Conv2d-39          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-40          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-41          [-1, 512, 14, 14]               0\n",
      "       DoubleConv-42          [-1, 512, 14, 14]               0\n",
      "             Down-43          [-1, 512, 14, 14]               0\n",
      "           Conv2d-44          [-1, 768, 14, 14]         393,984\n",
      "   PatchEmbedding-45             [-1, 196, 768]               0\n",
      "        LayerNorm-46             [-1, 196, 768]           1,536\n",
      "MultiheadAttention-47  [[-1, 196, 768], [-1, 196, 196]]               0\n",
      "        LayerNorm-48             [-1, 196, 768]           1,536\n",
      "           Linear-49            [-1, 196, 3072]       2,362,368\n",
      "             GELU-50            [-1, 196, 3072]               0\n",
      "          Dropout-51            [-1, 196, 3072]               0\n",
      "           Linear-52             [-1, 196, 768]       2,360,064\n",
      "          Dropout-53             [-1, 196, 768]               0\n",
      " TransformerLayer-54             [-1, 196, 768]               0\n",
      "        LayerNorm-55             [-1, 196, 768]           1,536\n",
      "MultiheadAttention-56  [[-1, 196, 768], [-1, 196, 196]]               0\n",
      "        LayerNorm-57             [-1, 196, 768]           1,536\n",
      "           Linear-58            [-1, 196, 3072]       2,362,368\n",
      "             GELU-59            [-1, 196, 3072]               0\n",
      "          Dropout-60            [-1, 196, 3072]               0\n",
      "           Linear-61             [-1, 196, 768]       2,360,064\n",
      "          Dropout-62             [-1, 196, 768]               0\n",
      " TransformerLayer-63             [-1, 196, 768]               0\n",
      "        LayerNorm-64             [-1, 196, 768]           1,536\n",
      "MultiheadAttention-65  [[-1, 196, 768], [-1, 196, 196]]               0\n",
      "        LayerNorm-66             [-1, 196, 768]           1,536\n",
      "           Linear-67            [-1, 196, 3072]       2,362,368\n",
      "             GELU-68            [-1, 196, 3072]               0\n",
      "          Dropout-69            [-1, 196, 3072]               0\n",
      "           Linear-70             [-1, 196, 768]       2,360,064\n",
      "          Dropout-71             [-1, 196, 768]               0\n",
      " TransformerLayer-72             [-1, 196, 768]               0\n",
      "        LayerNorm-73             [-1, 196, 768]           1,536\n",
      "MultiheadAttention-74  [[-1, 196, 768], [-1, 196, 196]]               0\n",
      "        LayerNorm-75             [-1, 196, 768]           1,536\n",
      "           Linear-76            [-1, 196, 3072]       2,362,368\n",
      "             GELU-77            [-1, 196, 3072]               0\n",
      "          Dropout-78            [-1, 196, 3072]               0\n",
      "           Linear-79             [-1, 196, 768]       2,360,064\n",
      "          Dropout-80             [-1, 196, 768]               0\n",
      " TransformerLayer-81             [-1, 196, 768]               0\n",
      "           Conv2d-82          [-1, 512, 14, 14]         393,728\n",
      "         Upsample-83          [-1, 512, 28, 28]               0\n",
      "           Conv2d-84          [-1, 256, 28, 28]       1,179,648\n",
      "      BatchNorm2d-85          [-1, 256, 28, 28]             512\n",
      "             ReLU-86          [-1, 256, 28, 28]               0\n",
      "       SingleConv-87          [-1, 256, 28, 28]               0\n",
      "           Conv2d-88          [-1, 256, 28, 28]       1,179,648\n",
      "      BatchNorm2d-89          [-1, 256, 28, 28]             512\n",
      "             ReLU-90          [-1, 256, 28, 28]               0\n",
      "       SingleConv-91          [-1, 256, 28, 28]               0\n",
      "               Up-92          [-1, 256, 28, 28]               0\n",
      "         Upsample-93          [-1, 256, 56, 56]               0\n",
      "           Conv2d-94          [-1, 128, 56, 56]         294,912\n",
      "      BatchNorm2d-95          [-1, 128, 56, 56]             256\n",
      "             ReLU-96          [-1, 128, 56, 56]               0\n",
      "       SingleConv-97          [-1, 128, 56, 56]               0\n",
      "           Conv2d-98          [-1, 128, 56, 56]         294,912\n",
      "      BatchNorm2d-99          [-1, 128, 56, 56]             256\n",
      "            ReLU-100          [-1, 128, 56, 56]               0\n",
      "      SingleConv-101          [-1, 128, 56, 56]               0\n",
      "              Up-102          [-1, 128, 56, 56]               0\n",
      "        Upsample-103        [-1, 128, 112, 112]               0\n",
      "          Conv2d-104         [-1, 64, 112, 112]          73,728\n",
      "     BatchNorm2d-105         [-1, 64, 112, 112]             128\n",
      "            ReLU-106         [-1, 64, 112, 112]               0\n",
      "      SingleConv-107         [-1, 64, 112, 112]               0\n",
      "          Conv2d-108         [-1, 64, 112, 112]          73,728\n",
      "     BatchNorm2d-109         [-1, 64, 112, 112]             128\n",
      "            ReLU-110         [-1, 64, 112, 112]               0\n",
      "      SingleConv-111         [-1, 64, 112, 112]               0\n",
      "              Up-112         [-1, 64, 112, 112]               0\n",
      "        Upsample-113         [-1, 64, 224, 224]               0\n",
      "          Conv2d-114         [-1, 16, 224, 224]           9,216\n",
      "     BatchNorm2d-115         [-1, 16, 224, 224]              32\n",
      "            ReLU-116         [-1, 16, 224, 224]               0\n",
      "      SingleConv-117         [-1, 16, 224, 224]               0\n",
      "              Up-118         [-1, 16, 224, 224]               0\n",
      "          Conv2d-119         [-1, 16, 224, 224]           2,304\n",
      "     BatchNorm2d-120         [-1, 16, 224, 224]              32\n",
      "            ReLU-121         [-1, 16, 224, 224]               0\n",
      "      SingleConv-122         [-1, 16, 224, 224]               0\n",
      "          Conv2d-123          [-1, 1, 224, 224]              17\n",
      "         OutConv-124          [-1, 1, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 27,513,905\n",
      "Trainable params: 27,513,905\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 176010.68\n",
      "Params size (MB): 104.96\n",
      "Estimated Total Size (MB): 176116.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.model.transunet import TransUnet\n",
    "from torchsummary import summary\n",
    "\n",
    "fig_size = (224, 224)\n",
    "model = TransUnet(\n",
    "    n_channels=3, \n",
    "    n_classes=1,\n",
    "    embed_dim=768,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    mlp_dim=3072,\n",
    "    dropout_rate=0.1,\n",
    "    fig_size=fig_size\n",
    ")\n",
    "\n",
    "summary(model, (3, fig_size[0], fig_size[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c834b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09bd90cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4628b0ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
