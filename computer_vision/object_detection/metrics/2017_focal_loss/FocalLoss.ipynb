{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d4af2a",
   "metadata": {},
   "source": [
    "## 交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75316b97",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{CE}(p, y) &= -\\sum_{i}^C y_i \\log(p_i) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "其中:\n",
    "- $C$是类别总数\n",
    "- $p_i$是模型预测的第$i$类的概率\n",
    "- $y_i$是实际标签的第$i$类的指示变量（1表示正确类别，0表示其他类别）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2328ce",
   "metadata": {},
   "source": [
    "### 二元交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddfb7a",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{CE}(p, y) &= \\begin{cases}\n",
    "-\\log(p) & \\text{if } y = 1 \\\\\n",
    "-\\log(1 - p) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "p_t &= \\begin{cases}\n",
    "p & \\text{if } y = 1 \\\\\n",
    "1 - p & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "\\text{CE}(p_t) &= -\\log(p_t) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97226e15",
   "metadata": {},
   "source": [
    "### 平衡交叉熵\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7d047",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{CE}(p_t) &= -\\alpha_t \\log(p_t) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "其中:\n",
    "- $\\alpha_t$是类别平衡因子，通常使用类别频率的倒数进行设置，也可以使用交叉验证来确定最佳值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3338a24",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f65e95",
   "metadata": {},
   "source": [
    "- $\\alpha$只关注类别数量的区别，并没有关注区分样本本身的难易程度。\n",
    "- 比如某些背景元素，在训练的过程中置信度极高（$p_t \\approx 1$, 对应CE=$-\\log(p_t) \\approx 0$），很容易被分类，但由于数量较多，导致他们的和在总损失中占据了很大比例（主导了梯度），影响了模型的训练效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed028a32",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\text{FL}(p_t) &= - (1 - p_t)^\\gamma \\log(p_t) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49adf1",
   "metadata": {},
   "source": [
    "- $\\gamma$是调制因子，[图 1](#fig_speed)中可视化多个值$\\gamma \\in [0, 5]$的效果，观察到两个特性：\n",
    "  - 当一个例子被错误分类且$p_t$较小的时候，调制因子$(1 - p_t)^\\gamma$接近于1，对损失影响较小。随着$p_t$增大(接近1)，调制因子迅速下降，降低了易分类样本($p_t$大)的损失贡献。\n",
    "  - $\\gamma$降低易分类样本权重的速度比较平滑。当$\\gamma=0$时，FL等价于CE。随着$\\gamma$的增大，调制因子带来的影响也会增大。(实验中$\\gamma=2$最佳)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5ca8d",
   "metadata": {},
   "source": [
    "<div style=\"background: white; padding: 10px; border-radius: 5px; margin: auto; width: 60%; \">\n",
    "    <a id=\"fig_speed\"></a>\n",
    "    <image src=\"./assets/speed.png\" alt=\"Focal Loss Modulating Factor\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585aa63",
   "metadata": {},
   "source": [
    "实际中使用的是$\\alpha$平衡的Focal Loss:\n",
    "$$\\begin{aligned}\n",
    "\\text{FL}(p_t) &= - \\alpha_t (1 - p_t) ^\\gamma \\log(p_t) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15648d8a",
   "metadata": {},
   "source": [
    "## 消融实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba25eb",
   "metadata": {},
   "source": [
    "<div style=\"background: white; padding: 10px; border-radius: 5px; margin: auto; width: 60%; \">\n",
    "    <image src=\"./assets/ablation.png\" alt=\"Focal Loss Ablation Study\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e1f49",
   "metadata": {},
   "source": [
    "## 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd7861c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本真实类别:\n",
      " [1 4 2 1 0 0 0 3 1 2 0 4 3 0 0 0 0 1 1 0 1 0 0 1 1 2 0 1 1 0 1 0 0 4 4 3 0\n",
      " 0 2 1 0 1 0 3 0 2 0 1 1 0 4 2 4 3 1 4 0 0 0 0 1 0 3 0]\n",
      "模型输出logits(只显示前5个):\n",
      " [[ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922  ]\n",
      " [ 0.61167629  1.03099952  0.93128012 -0.83921752 -0.30921238]\n",
      " [ 0.33126343  0.97554513 -0.47917424 -0.18565898 -1.10633497]\n",
      " [-1.19620662  0.81252582  1.35624003 -0.07201012  1.0035329 ]\n",
      " [ 0.36163603 -0.64511975  0.36139561  1.53803657 -0.03582604]]\n",
      "模型输出概率(只显示前5个):\n",
      " [[0.33953151 0.04130189 0.33296335 0.16383604 0.12236721]\n",
      " [0.2207486  0.33574359 0.30387862 0.0517348  0.08789438]\n",
      " [0.23905531 0.45530915 0.10629915 0.14256136 0.05677503]\n",
      " [0.02994662 0.22321804 0.38446903 0.09216801 0.2701983 ]\n",
      " [0.15923905 0.05818635 0.15920077 0.51636147 0.10701237]]\n",
      "类别平衡因子alpha: [0.06257669 0.10306748 0.29202455 0.29202455 0.25030676]\n",
      "应用alpha加权之前各类别对CE损失的贡献: [52.79996377 38.62728709 12.29027883  6.42584673 13.02320717]\n",
      "ce_loss: 1.9244778687028559\n",
      "应用alpha加权之后各类别对CE损失的贡献: [3.3040469  3.98121714 3.58906318 1.87650502 3.25979673]\n",
      "weighted ce_loss: 0.2501660776925498\n",
      "应用alpha加权之前各类别对FL损失的贡献: [39.56126889 31.6393047   9.43679129  3.33682212  9.37924909]\n",
      "fl_loss: 1.4586474388200799\n",
      "应用alpha加权之后各类别对FL损失的贡献: [2.47561321 3.26098341 2.75577476 0.97443399 2.34768941]\n",
      "weighted fl_loss: 0.1846014807596851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 设置np.random.seed以确保结果可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "def my_softmax(logits):  # (B, C)\n",
    "    max_vals = np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - max_vals)  # 减去max_vals将他们都变成负数\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "def my_cross_entropy_loss(logits, targets, alpha=None):\n",
    "    probs = my_softmax(logits)\n",
    "    batch_size = logits.shape[0]\n",
    "    eps = 1e-12\n",
    "    true_probs = probs[np.arange(batch_size), targets] + eps  # [0.04130189 0.08789438 0.10629915 0.22321804 0.15923905, ...]\n",
    "    ce_loss = -np.log(true_probs)  # 只考虑true class的概率\n",
    "    # 按类别相加，计算贡献\n",
    "    num_classes = len(np.unique(targets))\n",
    "    class_contributions = np.zeros(num_classes)\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (targets == cls)\n",
    "        class_contributions[cls] = ce_loss[cls_mask].sum()\n",
    "    print(\"应用alpha加权之前各类别对CE损失的贡献:\", class_contributions)\n",
    "    print(\"ce_loss:\", ce_loss.mean())\n",
    "    if alpha is not None:\n",
    "        alpha = np.array(alpha)\n",
    "        ce_loss = alpha[targets] * ce_loss\n",
    "        for cls in range(num_classes):\n",
    "            cls_mask = (targets == cls)\n",
    "            class_contributions[cls] = ce_loss[cls_mask].sum()\n",
    "        print(\"应用alpha加权之后各类别对CE损失的贡献:\", class_contributions)\n",
    "    print(\"weighted ce_loss:\", ce_loss.mean())\n",
    "    return ce_loss.mean()\n",
    "def my_focal_loss(logits, targets, gamma=2.0, alpha=None):\n",
    "    probs = my_softmax(logits)\n",
    "    batch_size = logits.shape[0]\n",
    "    eps = 1e-12\n",
    "    pt = probs[np.arange(batch_size), targets] + eps  # [0.04130189 0.08789438 0.10629915 0.22321804 0.15923905, ...]\n",
    "    fl_core = - np.log(pt) * (1 - pt) ** gamma\n",
    "    num_classes = len(np.unique(targets))\n",
    "    class_contributions = np.zeros(num_classes)\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (targets == cls)\n",
    "        class_contributions[cls] = fl_core[cls_mask].sum()\n",
    "    print(\"应用alpha加权之前各类别对FL损失的贡献:\", class_contributions)\n",
    "    print(\"fl_loss:\", fl_core.mean())\n",
    "    if alpha is not None:\n",
    "        alpha = np.array(alpha)\n",
    "        fl_core = alpha[targets] * fl_core\n",
    "        for cls in range(num_classes):\n",
    "            cls_mask = (targets == cls)\n",
    "            class_contributions[cls] = fl_core[cls_mask].sum()\n",
    "        print(\"应用alpha加权之后各类别对FL损失的贡献:\", class_contributions)\n",
    "    print(\"weighted fl_loss:\", fl_core.mean())\n",
    "    return fl_core.mean()\n",
    "\n",
    "n_classes = 5\n",
    "batch_size = 64\n",
    "imbalance_ratio = 0.3\n",
    "class_probs = np.array([(1 - imbalance_ratio) ** i for i in range(n_classes)])  # [0.36060726 0.25242508 0.17669756 0.12368829 0.0865818 ]\n",
    "class_probs /= class_probs.sum()  # 归一化\n",
    "targets = np.random.choice(n_classes, size = batch_size, p=class_probs)\n",
    "print(\"样本真实类别:\\n\", targets)\n",
    "logits = np.random.randn(batch_size, n_classes)\n",
    "print(\"模型输出logits(只显示前5个):\\n\", logits[:5])\n",
    "probs = my_softmax(logits)\n",
    "print(\"模型输出概率(只显示前5个):\\n\", probs[:5])\n",
    "\n",
    "class_counts = np.bincount(targets, minlength=n_classes)\n",
    "class_counts = [count if count > 0 else 1 for count in class_counts]\n",
    "alpha = 1.0 / np.array(class_counts, dtype=np.float32)\n",
    "alpha /= alpha.sum()  # 归一化\n",
    "print(\"类别平衡因子alpha:\", alpha)\n",
    "\n",
    "ce_loss = my_cross_entropy_loss(logits, targets, alpha=alpha)\n",
    "# print(\"ce_loss:\", ce_loss)\n",
    "fl_loss = my_focal_loss(logits, targets, gamma=2.0, alpha=alpha)\n",
    "# print(\"fl_loss:\", fl_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
