## 问题描述
> IndexError: piece id is out of range.

- 精简版的错误堆栈信息：

```shell
File "/root/tasks/train.py", line 160, in <module>
    trainer.train()
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 2790, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 3221, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 3170, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 4489, in evaluate
    output = eval_loop(
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/trainer.py", line 4780, in evaluation_loop
    metrics = self.compute_metrics(
  File "/root/tasks/train.py", line 115, in compute_metrics
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3884, in batch_decode
    return [
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3885, in <listcomp>
    self.decode(
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3924, in decode
    return self._decode(
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 1092, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 1073, in convert_ids_to_tokens
    tokens.append(self._convert_id_to_token(index))
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py", line 404, in _convert_id_to_token
    token = self.sp_model.IdToPiece(index)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/sentencepiece/__init__.py", line 1183, in _batched_func
    return _func(self, arg)
  File "/root/miniconda3/envs/flan_t5/lib/python3.10/site-packages/sentencepiece/__init__.py", line 1176, in _func
    raise IndexError('piece id is out of range.')
IndexError: piece id is out of range.
 33%|███▎      | 7645/22935 [34:17<1:08:35,  3.72it/s] 

```

- 完整版的错误堆栈信息：
```shell
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[15], line 1
----> 1 trainer.train()

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2325, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   2323         hf_hub_utils.enable_progress_bars()
   2324 else:
-> 2325     return inner_training_loop(
   2326         args=args,
   2327         resume_from_checkpoint=resume_from_checkpoint,
   2328         trial=trial,
   2329         ignore_keys_for_eval=ignore_keys_for_eval,
   2330     )

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2790, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2787     self.control.should_training_stop = True
   2789 self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
-> 2790 self._maybe_log_save_evaluate(
   2791     tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=learning_rate
   2792 )
   2794 if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
   2795     if is_torch_xla_available():
   2796         # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3221, in Trainer._maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)
   3219 metrics = None
   3220 if self.control.should_evaluate:
-> 3221     metrics = self._evaluate(trial, ignore_keys_for_eval)
   3222     is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
   3224     if self.args.save_strategy == SaveStrategy.BEST:

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3170, in Trainer._evaluate(self, trial, ignore_keys_for_eval, skip_scheduler)
   3169 def _evaluate(self, trial, ignore_keys_for_eval, skip_scheduler=False):
-> 3170     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
   3171     self._report_to_hp_search(trial, self.state.global_step, metrics)
   3173     # Run delayed LR scheduler now that metrics are populated

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:191, in Seq2SeqTrainer.evaluate(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)
    189 self.gather_function = self.accelerator.gather
    190 self._gen_kwargs = gen_kwargs
--> 191 return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:4489, in Trainer.evaluate(self, eval_dataset, ignore_keys, metric_key_prefix)
   4486 start_time = time.time()
   4488 eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-> 4489 output = eval_loop(
   4490     eval_dataloader,
   4491     description="Evaluation",
   4492     # No point gathering the predictions if there are no metrics, otherwise we defer to
   4493     # self.args.prediction_loss_only
   4494     prediction_loss_only=True if self.compute_metrics is None else None,
   4495     ignore_keys=ignore_keys,
   4496     metric_key_prefix=metric_key_prefix,
   4497 )
   4499 total_batch_size = self.args.eval_batch_size * self.args.world_size
   4500 if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:

File ~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:4780, in Trainer.evaluation_loop(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)
   4778     eval_set_kwargs["losses"] = all_losses if "loss" in args.include_for_metrics else None
   4779     eval_set_kwargs["inputs"] = all_inputs if "inputs" in args.include_for_metrics else None
-> 4780     metrics = self.compute_metrics(
   4781         EvalPrediction(predictions=all_preds, label_ids=all_labels, **eval_set_kwargs)
   4782     )
   4783 elif metrics is None:
   4784     metrics = {}

Cell In[11], line 10, in compute_metrics(eval_preds)
      8 # decode preds and labels
      9 labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
---> 10 decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
     11 decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
     13 # 初始化 ROUGE scorer

File ~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3885, in PreTrainedTokenizerBase.batch_decode(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3860 def batch_decode(
   3861     self,
   3862     sequences: Union[list[int], list[list[int]], "np.ndarray", "torch.Tensor", "tf.Tensor"],
   (...)   3865     **kwargs,
   3866 ) -> list[str]:
   3867     """
   3868     Convert a list of lists of token ids into a list of strings by calling decode.
   3869 
   (...)   3882         `list[str]`: The list of decoded sentences.
   3883     """
   3884     return [
-> 3885         self.decode(
   3886             seq,
   3887             skip_special_tokens=skip_special_tokens,
   3888             clean_up_tokenization_spaces=clean_up_tokenization_spaces,
   3889             **kwargs,
   3890         )
   3891         for seq in sequences
   3892     ]

File ~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3924, in PreTrainedTokenizerBase.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3921 # Convert inputs to python lists
   3922 token_ids = to_py_obj(token_ids)
-> 3924 return self._decode(
   3925     token_ids=token_ids,
   3926     skip_special_tokens=skip_special_tokens,
   3927     clean_up_tokenization_spaces=clean_up_tokenization_spaces,
   3928     **kwargs,
   3929 )

File ~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils.py:1092, in PreTrainedTokenizer._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)
   1082 def _decode(
   1083     self,
   1084     token_ids: Union[int, list[int]],
   (...)   1088     **kwargs,
   1089 ) -> str:
   1090     self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)
-> 1092     filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
   1093     # If given is a single id, prevents splitting the string in upcoming loop
   1094     if isinstance(filtered_tokens, str):

File ~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils.py:1073, in PreTrainedTokenizer.convert_ids_to_tokens(self, ids, skip_special_tokens)
   1071         tokens.append(self._added_tokens_decoder[index].content)
   1072     else:
-> 1073         tokens.append(self._convert_id_to_token(index))
   1074 return tokens

File ~/miniconda3/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5.py:404, in T5Tokenizer._convert_id_to_token(self, index)
    402 def _convert_id_to_token(self, index):
    403     """Converts an index (integer) in a token (str) using the vocab."""
--> 404     token = self.sp_model.IdToPiece(index)
    405     return token

File ~/miniconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1183, in _batchnize.<locals>._batched_func(self, arg)
   1181   return [_func(self, n) for n in arg]
   1182 else:
-> 1183   return _func(self, arg)

File ~/miniconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1176, in _batchnize.<locals>._func(v, n)
   1174 def _func(v, n):
   1175   if type(n) is int and (n < 0 or n >= v.piece_size()):
-> 1176     raise IndexError('piece id is out of range.')
   1177   return func(v, n)

IndexError: piece id is out of range.
```

## 问题分析

- 问题原因：模型在解码预测结果时(`preds`)，传入了一个超出词表范围的piece id，导致IndexError异常。

## 解决方案
```py
# SOL: 新增对preds的预处理，确保preds中的ID在有效范围内(preds可能包含超出范围的ID)
preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
vocab_size = tokenizer.vocab_size
preds = np.clip(preds, 0, vocab_size - 1)

# NT: 移除-100，因为-100是标签中的ignore_index，不应出现在预测结果中
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
# rougeLSum expects newline after each sentence
decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

# HL: 因为传入的是decode的结果，所以compute计算中可以用nltk的encoder
result = metric.nltk_rouge_compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
return result
```

- 样例`result`结果

```py
{'eval_loss': 3.1135520935058594, 'eval_rouge1': 0.1497, 'eval_rouge2': 0.0234, 'eval_rougeL': 0.1105, 'eval_rougeLsum': 0.1105, 'eval_runtime': 1797.2142, 'eval_samples_per_second': 14.583, 'eval_steps_per_second': 3.646, 'epoch': 3.0}
```