## 问题描述

> RuntimeError: stack expects each tensor to be equal size, but got [3, 320, 416] at entry 0 and [3, 320, 480] at entry 1

- 运行时出现错误：栈期望每个张量大小相等，但在第0个条目中得到[3, 320, 416]，在第1个条目中得到[3, 320, 480]。

- 精简版的堆栈信息

```sh
RuntimeError: stack expects each tensor to be equal size, but got [3, 320, 416] at entry 0 and [3, 320, 480] at entry 1
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
```


- 详细的堆栈信息
```sh
2025-12-06 23:42:59,403 - ERROR - Training failed with error: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 211, in collate
    return [
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 212, in <listcomp>
    collate(samples, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 155, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 272, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [3, 320, 416] at entry 0 and [3, 320, 480] at entry 1
Traceback (most recent call last):
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\train.py", line 602, in <module>
    main(args)
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\train.py", line 417, in main
    train_loss = train_one_epoch(
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\trainer.py", line 34, in train_one_epoch
    for batch_idx, (images, masks) in enumerate(pbar):
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 732, in __next__
    data = self._next_data()
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 1506, in _next_data
    return self._process_data(data, worker_id)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 1541, in _process_data
    data.reraise()
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\_utils.py", line 769, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 211, in collate
    return [
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 212, in <listcomp>
    collate(samples, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 155, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 272, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [3, 320, 416] at entry 0 and [3, 320, 480] at entry 1

Traceback (most recent call last):
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\train.py", line 602, in <module>
    main(args)
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\train.py", line 417, in main
    train_loss = train_one_epoch(
  File "C:\Users\Alvis\OneDrive - Office365\ace\zijie\task3\unet-pet-segmentation\trainer.py", line 34, in train_one_epoch
    for batch_idx, (images, masks) in enumerate(pbar):
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\tqdm\std.py", line 1181, in __iter__
    for obj in iterable:
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 732, in __next__
    data = self._next_data()
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 1506, in _next_data
    return self._process_data(data, worker_id)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\dataloader.py", line 1541, in _process_data
    data.reraise()
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\_utils.py", line 769, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 211, in collate
    return [
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 212, in <listcomp>
    collate(samples, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 155, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\ProgramData\miniconda3\envs\unet\lib\site-packages\torch\utils\data\_utils\collate.py", line 272, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [3, 320, 416] at entry 0 and [3, 320, 480] at entry 1
```


## 问题分析
- 相关部分代码
```py
val_loader = DataLoader(
    val_dataset,
    batch_size=1,  # 由于测试集图片大小不一致，所以设置 batch_size=1
    shuffle=False,
    num_workers=1,  # 使用单线程加载数据
    pin_memory=True
)
```

- 问题原因：
  - 在使用 `PyTorch` 的 `DataLoader` 时，尝试将不同大小的张量堆叠在一起时会导致错误。
  - 但我设置`batch_size=1`，理论上每个批次只有一个样本，应该不会出现这个问题。
  - 原因在于`num_works=1`使用子线程加载数据，默认的`collate`函数会检查batch中所有张量的形状是否一致

总结：问题出现在多个验证样本之间。虽然每个batch只有1个样本，但DataLoader会从多个worker并行加载多个样本，然后在collate阶段合并。当不同worker加载的样本经过不同尺寸的裁剪后，它们最终的尺寸不同，导致collate失败。

## 解决方案
- 方法1: 设置`num_workers=0`，这样就不会使用子线程加载数据，所有数据都在主线程中处理，避免了不同worker之间的尺寸不一致问题。
- 方法2: 自定义`collate_fn`函数.

- 方法1示例:
```py
val_loader = DataLoader(
    val_dataset,
    batch_size=1,  # 由于测试集图片大小不一致，所以设置 batch_size=1
    shuffle=False,
    num_workers=0,  # 禁用子线程，在主线程中加载数据
    pin_memory=True
)
```